#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±ä¸€æµ·äº‹è­¦å‘Šç›£æ§ç³»çµ± (ä¸­åœ‹æµ·äº‹å±€ + å°ç£èˆªæ¸¯å±€ + UKMTO)
æ”¯æ´ç¶“ç·¯åº¦æå–ã€Teams é€šçŸ¥ã€Email å ±å‘Š
ç‰ˆæœ¬: 3.1 - UKMTO åº§æ¨™æ”¹å¾ __NEXT_DATA__ ç›´æ¥è®€å–
"""

import platform
import subprocess
import os
import sys
import ssl
import logging
import warnings
import json
import smtplib
import requests
import traceback
import re
import time
from datetime import datetime, timezone, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import urllib3
from database_manager import DatabaseManager
from keyword_manager import KeywordManager

# ==================== 1. å…¨åŸŸåˆå§‹åŒ– ====================
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['WDM_SSL_VERIFY'] = '0'
load_dotenv()
warnings.filterwarnings('ignore')
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logging.getLogger('selenium').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)

if os.name == 'nt':
    class ErrorFilter:
        def __init__(self, stream):
            self.stream = stream
        def write(self, text):
            if any(k in text for k in ['ERROR:net', 'handshake failed', 'DEPRECATED_ENDPOINT']):
                return
            self.stream.write(text)
        def flush(self):
            self.stream.flush()
    sys.stderr = ErrorFilter(sys.stderr)


# ==================== 2. åº§æ¨™æå–å™¨ (å¢å¼·ç‰ˆ) ====================
class CoordinateExtractor:
    def __init__(self):
        self.patterns = [
            r'(\d{1,3})-(\d{1,2}\.\d+)\s*([NSnsåŒ—å—])\s+(\d{1,3})-(\d{1,2}\.\d+)\s*([EWewæ±è¥¿])',
            r'(\d{1,3})-(\d{1,2})\s*([NSnsåŒ—å—])\s+(\d{1,3})-(\d{1,2})\s*([EWewæ±è¥¿])',
            r'(\d{1,3})[Â°åº¦]\s*(\d{1,2})[\'â€²åˆ†]?\s*([NSnsåŒ—å—])\s+(\d{1,3})[Â°åº¦]\s*(\d{1,2})[\'â€²åˆ†]?\s*([EWewæ±è¥¿])',
            r'(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?\s*([NSnsåŒ—å—])\s+(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?\s*([EWewæ±è¥¿])',
            r'([NSnsåŒ—å—])\s*(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?\s+([EWewæ±è¥¿])\s*(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?',
            r'(\d{1,3}\.\d+)\s*[Â°åº¦]?\s*([NSnsåŒ—å—])\s+(\d{1,3}\.\d+)\s*[Â°åº¦]?\s*([EWewæ±è¥¿])',
            r'[åŒ—å—ç·¯]\s*(\d{1,3})\s*åº¦\s*(\d{1,2})\s*åˆ†\s+[æ±è¥¿ç¶“]\s*(\d{1,3})\s*åº¦\s*(\d{1,2})\s*åˆ†',
        ]
        print("  ğŸ—ºï¸ åº§æ¨™æå–å™¨åˆå§‹åŒ–å®Œæˆ")

    def extract_coordinates(self, text):
        coordinates = []
        if not text:
            return coordinates
        text = text.replace('ã€', ' ').replace('ï¼Œ', ' ').replace('ã€‚', ' ')
        for pattern in self.patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    coord = self._parse_match(match, pattern)
                    if coord and self._validate_coordinate(coord):
                        coordinates.append(coord)
                except Exception:
                    continue
        unique_coords = []
        for coord in coordinates:
            is_duplicate = False
            for existing in unique_coords:
                if abs(coord[0] - existing[0]) < 0.01 and abs(coord[1] - existing[1]) < 0.01:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_coords.append(coord)
        return unique_coords

    def _parse_match(self, match, pattern):
        groups = match.groups()
        if len(groups) == 4 and '\\.' in pattern and 'degree' not in pattern:
            try:
                lat = float(groups[0])
                lat_dir = groups[1].upper()
                lon = float(groups[2])
                lon_dir = groups[3].upper()
                if lat_dir in ['S', 's', 'å—']:
                    lat = -lat
                if lon_dir in ['W', 'w', 'è¥¿']:
                    lon = -lon
                return (lat, lon)
            except:
                return None
        if len(groups) >= 6 and groups[0] in ['N', 'S', 'n', 's', 'åŒ—', 'å—']:
            try:
                lat_dir = groups[0].upper()
                lat_deg = float(groups[1])
                lat_min = float(groups[2])
                lon_dir = groups[3].upper()
                lon_deg = float(groups[4])
                lon_min = float(groups[5])
                lat = lat_deg + lat_min / 60
                lon = lon_deg + lon_min / 60
                if lat_dir in ['S', 's', 'å—']:
                    lat = -lat
                if lon_dir in ['W', 'w', 'è¥¿']:
                    lon = -lon
                return (lat, lon)
            except:
                return None
        if len(groups) >= 6:
            try:
                lat_deg = float(groups[0])
                lat_min = float(groups[1])
                lat_dir = groups[2].upper() if len(groups[2]) > 0 else 'N'
                lon_deg = float(groups[3])
                lon_min = float(groups[4])
                lon_dir = groups[5].upper() if len(groups[5]) > 0 else 'E'
                lat = lat_deg + lat_min / 60
                lon = lon_deg + lon_min / 60
                if lat_dir in ['S', 's', 'å—']:
                    lat = -lat
                if lon_dir in ['W', 'w', 'è¥¿']:
                    lon = -lon
                return (lat, lon)
            except:
                return None
        return None

    def _validate_coordinate(self, coord):
        if not coord or len(coord) != 2:
            return False
        lat, lon = coord
        if lat < -90 or lat > 90:
            return False
        if lon < -180 or lon > 180:
            return False
        if not (-60 <= lat <= 60 and 60 <= lon <= 180):
            return False
        return True

    def extract_from_html(self, html_content):
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            content_div = soup.find('div', {'class': 'text', 'id': 'ch_p'})
            if content_div:
                return self.extract_coordinates(content_div.get_text())
            return self.extract_coordinates(html_content)
        except Exception as e:
            print(f"    âš ï¸ HTML è§£æå¤±æ•—: {e}")
            return []

    def format_coordinates(self, coordinates):
        if not coordinates:
            return "ç„¡åº§æ¨™è³‡è¨Š"
        formatted = []
        for lat, lon in coordinates:
            lat_dir = 'N' if lat >= 0 else 'S'
            lon_dir = 'E' if lon >= 0 else 'W'
            formatted.append(f"{abs(lat):.4f}Â°{lat_dir}, {abs(lon):.4f}Â°{lon_dir}")
        return " | ".join(formatted)


# ==================== 3. çµ±ä¸€ Teams é€šçŸ¥ç³»çµ± ====================
class UnifiedTeamsNotifier:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url

    def _fix_url(self, url, base_domain=""):
        if not url:
            return base_domain or "https://www.msa.gov.cn/page/outter/weather.jsp"
        url = url.strip()
        if url.startswith('/'):
            return f"{base_domain}{url}" if base_domain else f"https://www.msa.gov.cn{url}"
        if url.startswith(('http://', 'https://')):
            return url
        if url.startswith(('javascript:', '#')):
            return base_domain or "https://www.msa.gov.cn/page/outter/weather.jsp"
        return f"{base_domain}/{url}" if base_domain else f"https://www.msa.gov.cn/{url}"

    def _create_adaptive_card(self, title, body_elements, actions=None):
        card_content = {
            "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
            "type": "AdaptiveCard",
            "version": "1.4",
            "body": [{"type": "TextBlock", "text": title, "weight": "Bolder", "size": "Large", "color": "Attention"}] + body_elements
        }
        if actions:
            card_content["actions"] = actions
        return {
            "type": "message",
            "attachments": [{"contentType": "application/vnd.microsoft.card.adaptive", "contentUrl": None, "content": card_content}]
        }

    def send_batch_notification(self, warnings_list, source_type="CN_MSA", is_today=True):
        if not self.webhook_url or not warnings_list:
            return False
        try:
            source_config = {
                "TW_MPB": {"icon": "ğŸ‡¹ğŸ‡¼", "name": "å°ç£èˆªæ¸¯å±€",    "home_url": "https://www.motcmpb.gov.tw/Information/Notice?SiteId=1&NodeId=483", "base_domain": "https://www.motcmpb.gov.tw"},
                "UKMTO":  {"icon": "ğŸ‡¬ğŸ‡§", "name": "UKMTO èˆªè¡Œè­¦å‘Š","home_url": "https://www.ukmto.org/recent-incidents",                             "base_domain": "https://www.ukmto.org"},
                "CN_MSA": {"icon": "ğŸ‡¨ğŸ‡³", "name": "ä¸­åœ‹æµ·äº‹å±€",    "home_url": "https://www.msa.gov.cn/page/outter/weather.jsp",                     "base_domain": "https://www.msa.gov.cn"},
            }
            cfg         = source_config.get(source_type, source_config["CN_MSA"])
            source_icon = cfg["icon"]
            source_name = cfg["name"]
            home_url    = cfg["home_url"]
            base_domain = cfg["base_domain"]
            time_badge  = "ğŸ†• ä»Šæ—¥æ–°å¢" if is_today else "ğŸ“š æ­·å²è³‡æ–™ (è¿‘30å¤©)"
            title_color = "Attention" if is_today else "Good"

            body_elements = [
                {"type": "TextBlock", "text": f"{source_icon} **{source_name}** | {time_badge}", "size": "Medium", "weight": "Bolder", "color": title_color},
                {"type": "TextBlock", "text": f"ç™¼ç¾ **{len(warnings_list)}** å€‹èˆªè¡Œè­¦å‘Š", "size": "Medium"},
                {"type": "TextBlock", "text": "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”", "wrap": True}
            ]
            actions = []

            for idx, w in enumerate(warnings_list[:8], 1):
                _, bureau, title, link, pub_time, _, _, coordinates = w
                fixed_link = self._fix_url(link, base_domain)

                # â”€â”€ åº§æ¨™æ‘˜è¦ â”€â”€
                coord_summary = "ç„¡åº§æ¨™"
                if coordinates:
                    try:
                        coord_list = json.loads(coordinates) if isinstance(coordinates, str) else coordinates
                        if coord_list:
                            first   = coord_list[0]
                            lat, lon = first[0], first[1]
                            lat_dir  = 'N' if lat >= 0 else 'S'
                            lon_dir  = 'E' if lon >= 0 else 'W'
                            coord_summary = f"ğŸ“ {abs(lat):.4f}Â°{lat_dir}, {abs(lon):.4f}Â°{lon_dir}"
                            if len(coord_list) > 1:
                                coord_summary += f" (+{len(coord_list)-1})"
                    except:
                        coord_summary = "åº§æ¨™æ ¼å¼éŒ¯èª¤"

                # â”€â”€ çµ„è£å¡ç‰‡å…ƒç´  â”€â”€
                item_elements = [
                    {"type": "TextBlock", "text": f"**{idx}. {bureau}**",
                    "weight": "Bolder", "color": "Accent", "spacing": "Medium"},
                    {"type": "TextBlock", "text": title[:100], "wrap": True},
                ]

                # UKMTO å°ˆå±¬ï¼šé¡¯ç¤ºå®Œæ•´é€šå‘Šå…§å®¹
                # w æ˜¯ tuple æ™‚æ²’æœ‰ detailsï¼Œéœ€è¦å¾ all_captured å–å¾—
                # â†’ åœ¨ä¸»ç¨‹å¼ _to_teams_tuple() æŠŠ details æ”¾é€² tuple[6]ï¼ˆåŸæœ¬æ˜¯ç©ºå­—ä¸²ï¼‰
                details_text = w[6] if isinstance(w, (list, tuple)) and len(w) > 6 else ""
                if details_text and source_type == "UKMTO":
                    item_elements.append({
                        "type": "TextBlock",
                        "text": details_text,
                        "wrap": True,
                        "size": "Small",
                        "color": "Default",
                        "spacing": "Small"
                    })

                item_elements.append({
                    "type": "TextBlock",
                    "text": f"ğŸ“… {pub_time} | {coord_summary}",
                    "size": "Small",
                    "isSubtle": True
                })

                body_elements.extend(item_elements)

                if len(actions) < 4:
                    actions.append({"type": "Action.OpenUrl", "title": f"ğŸ“„ å…¬å‘Š {idx}", "url": fixed_link})

            if len(warnings_list) > 8:
                body_elements.append({"type": "TextBlock", "text": f"*...é‚„æœ‰ {len(warnings_list)-8} ç­†æœªé¡¯ç¤º*", "isSubtle": True})

            actions.append({"type": "Action.OpenUrl", "title": f"ğŸ  {source_name}é¦–é ", "url": home_url})

            card_title = f"{'ğŸš¨' if is_today else 'ğŸ“‹'} {source_name} - {time_badge} ({len(warnings_list)})"
            payload    = self._create_adaptive_card(card_title, body_elements, actions)

            print(f"  ğŸ“¤ æ­£åœ¨ç™¼é€ Teams é€šçŸ¥ [{time_badge}] åˆ°: {self.webhook_url[:50]}...")
            response = requests.post(self.webhook_url, json=payload, headers={"Content-Type": "application/json"}, timeout=30, verify=False)

            if response.status_code in [200, 202]:
                print(f"âœ… {source_name} Teams é€šçŸ¥ç™¼é€æˆåŠŸ [{time_badge}] ({len(warnings_list)} ç­†)")
                return True
            else:
                print(f"âŒ {source_name} Teams é€šçŸ¥å¤±æ•—: HTTP {response.status_code} | {response.text[:200]}")
                return False

        except requests.exceptions.SSLError as e:
            print(f"âŒ Teams SSL éŒ¯èª¤: {e}"); return False
        except requests.exceptions.Timeout as e:
            print(f"âŒ Teams é€£ç·šé€¾æ™‚: {e}"); return False
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ Teams é€£ç·šéŒ¯èª¤: {e}"); return False
        except Exception as e:
            print(f"âŒ Teams ç™¼é€å¤±æ•—: {e}"); traceback.print_exc(); return False


# ==================== 4. Email é€šçŸ¥ç³»çµ± ====================
class GmailRelayNotifier:
    def __init__(self, mail_user, mail_pass, target_email):
        self.mail_user    = mail_user
        self.mail_pass    = mail_pass
        self.target_email = target_email
        self.smtp_server  = "smtp.gmail.com"
        self.smtp_port    = 587
        if not all([mail_user, mail_pass, target_email]):
            print("âš ï¸ Email é€šçŸ¥æœªå®Œæ•´è¨­å®š"); self.enabled = False
        else:
            self.enabled = True; print("âœ… Email é€šçŸ¥ç³»çµ±å·²å•Ÿç”¨")

    def send_trigger_email(self, today_warnings, history_warnings):
        if not self.enabled:
            print("â„¹ï¸ Email é€šçŸ¥æœªå•Ÿç”¨"); return False
        try:
            msg = MIMEMultipart('related')
            total_count = len(today_warnings) + len(history_warnings)
            msg['Subject'] = (
                f"ğŸŒŠ èˆªè¡Œè­¦å‘Šç›£æ§å ±å‘Š - å…±{total_count}ç­† (ä»Šæ—¥{len(today_warnings)}ç­†) - "
                f"{(datetime.now(timezone.utc) + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')}(TPE)"
            )
            msg['From'] = self.mail_user
            msg['To']   = self.target_email
            msg_alt = MIMEMultipart('alternative')
            msg.attach(msg_alt)
            msg_alt.attach(MIMEText(self._generate_html_report(today_warnings, history_warnings), 'html', 'utf-8'))
            print(f"ğŸ“§ æ­£åœ¨ç™¼é€éƒµä»¶è‡³ {self.target_email}...")
            with smtplib.SMTP(self.smtp_server, self.smtp_port, timeout=30) as server:
                server.starttls()
                server.login(self.mail_user, self.mail_pass)
                server.send_message(msg)
            print("âœ… éƒµä»¶ç™¼é€æˆåŠŸ"); return True
        except Exception as e:
            print(f"âŒ éƒµä»¶ç™¼é€å¤±æ•—: {e}"); traceback.print_exc(); return False

    def _source_icon(self, source):
        return {"TW_MPB": "ğŸ‡¹ğŸ‡¼", "UKMTO": "ğŸ‡¬ğŸ‡§"}.get(source, "ğŸ‡¨ğŸ‡³")

    def _generate_html_report(self, today_warnings, history_warnings):
        total_count = len(today_warnings) + len(history_warnings)
        html = f"""
        <html><head><meta charset="utf-8">
        <style>
            body {{ font-family: 'Microsoft JhengHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
            .container {{ max-width: 1000px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
            h1 {{ color: #003366; border-bottom: 3px solid #0066cc; padding-bottom: 10px; }}
            h2 {{ color: #0066cc; margin-top: 30px; padding: 10px; background: #f0f8ff; border-left: 4px solid #0066cc; }}
            .summary {{ background: #e3f2fd; padding: 15px; border-radius: 5px; margin: 20px 0; }}
            .summary-item {{ display: inline-block; margin: 5px 15px 5px 0; font-weight: bold; }}
            .warning-item {{ background: #f9f9f9; padding: 15px; margin: 15px 0; border-left: 4px solid #0066cc; border-radius: 5px; }}
            .warning-item.today {{ border-left-color: #ff6b6b; background: #fff5f5; }}
            .warning-item.history {{ border-left-color: #51cf66; background: #f0fff4; }}
            .warning-title {{ font-weight: bold; color: #003366; font-size: 16px; }}
            .warning-meta {{ color: #666; font-size: 14px; margin-top: 5px; line-height: 1.8; }}
            .coordinates {{ background: #e8f4fd; padding: 12px; margin-top: 10px; border-radius: 5px;
                            font-family: 'Courier New', monospace; font-size: 13px; border: 1px solid #bee3f8; }}
            .coord-source-label {{ font-weight: bold; color: #2b6cb0; margin-bottom: 6px; }}
            .coord-item {{ margin: 4px 0; color: #2d3748; }}
            .coord-map-link {{ font-size: 12px; color: #3182ce; text-decoration: none; margin-left: 8px; }}
            .details-block {{background: #fffbea;border: 1px solid #f6e05e;border-left: 4px solid #d69e2e;padding: 10px 14px;margin-top: 10px;border-radius: 5px;font-size: 14px;color: #2d3748;line-height: 1.7;}}
            .footer {{ margin-top: 30px; padding-top: 20px; border-top: 1px solid #ddd; color: #666; font-size: 12px; text-align: center; }}
            .badge {{ display: inline-block; padding: 3px 8px; border-radius: 3px; font-size: 12px; font-weight: bold; margin-left: 10px; }}
            .badge.today {{ background: #ff6b6b; color: white; }}
            .badge.history {{ background: #51cf66; color: white; }}
            .source-tag {{ display: inline-block; padding: 2px 6px; border-radius: 3px; font-size: 11px;
                           background: #6c5ce7; color: white; margin-left: 6px; }}
        </style>
        </head>
        <body>
        <div class="container">
            <h1>ğŸŒŠ WHL_FRMæµ·äº‹è­¦å‘Šç›£æ§å ±å‘Š</h1>
            <div class="summary">
                <div class="summary-item">ğŸ“… å ±å‘Šæ™‚é–“ï¼š{(datetime.now(timezone.utc) + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')} (TPE)</div><br>
                <div class="summary-item">ğŸ“Š ç¸½è­¦å‘Šæ•¸ï¼š{total_count} ç­†</div>
                <div class="summary-item">ğŸ†• ä»Šæ—¥æ–°å¢ï¼š{len(today_warnings)} ç­†</div>
                <div class="summary-item">ğŸ“š æ­·å²è³‡æ–™ï¼š{len(history_warnings)} ç­†</div>
            </div>
        """

        def _render_warnings(warnings_list, badge_class, badge_label):
            result = ""
            for idx, w in enumerate(warnings_list, 1):
                source = w.get('source', '')
                icon   = self._source_icon(source)
                coords = w.get('coordinates', [])

                # â”€â”€ åº§æ¨™å€å¡Š â”€â”€
                coord_html = ""
                if coords:
                    coord_source  = w.get('coord_source', 'text')
                    source_label_map = {
                        'next_data': 'ğŸ“¡ ä¾†æºï¼š__NEXT_DATA__ (ç²¾ç¢º)',
                        'text':      'ğŸ“ ä¾†æºï¼šæ–‡å­—è§£æ',
                        'fallback':  'ğŸ”„ ä¾†æºï¼šFallback è§£æ',
                    }
                    source_label = source_label_map.get(coord_source, 'ğŸ“ åº§æ¨™è³‡è¨Š')
                    coord_html   = f'<div class="coordinates"><div class="coord-source-label">{source_label}</div>'
                    for i, pt in enumerate(coords, 1):
                        lat, lon = pt[0], pt[1]
                        lat_dir  = 'N' if lat >= 0 else 'S'
                        lon_dir  = 'E' if lon >= 0 else 'W'
                        maps_url = f"https://www.google.com/maps?q={lat:.6f},{lon:.6f}"
                        coord_html += (
                            f'<div class="coord-item">'
                            f'{i}. {abs(lat):.4f}Â°{lat_dir}, {abs(lon):.4f}Â°{lon_dir}'
                            f'<a class="coord-map-link" href="{maps_url}" target="_blank">ğŸ—ºï¸ åœ°åœ–</a>'
                            f'</div>'
                        )
                    coord_html += '</div>'

                # â”€â”€ UKMTO ç‰¹æœ‰æ¬„ä½ â”€â”€
                extra_meta = ""
                if source == "UKMTO":
                    colour      = w.get('colour', '')
                    colour_icon = "ğŸ”´" if colour == "Red" else "ğŸŸ¡"
                    extra_meta  = f"âš ï¸ è­¦ç¤ºç­‰ç´šï¼š{colour_icon} {colour}<br>"

                # â”€â”€ UKMTO å®Œæ•´é€šå‘Šå…§å®¹ â”€â”€
                details_html = ""
                if source == "UKMTO" and w.get('details'):
                    details_html = f"""
                        <div class="details-block">
                            <strong>ğŸ“„ é€šå‘Šå…§å®¹ï¼š</strong><br>
                            {w['details']}
                        </div>
                    """

                kw     = w.get('keywords', [])
                kw_str = ', '.join(kw) if isinstance(kw, list) else str(kw)

                result += f"""
                    <div class="warning-item {badge_class}">
                        <div class="warning-title">
                            {icon} {idx}. {w.get('title', 'N/A')}
                            <span class="badge {badge_class}">{badge_label}</span>
                            {'<span class="source-tag">UKMTO</span>' if source == 'UKMTO' else ''}
                        </div>
                        <div class="warning-meta">
                            ğŸ“‹ ç™¼å¸ƒå–®ä½ï¼š{w.get('bureau', 'N/A')}<br>
                            ğŸ“… ç™¼å¸ƒæ™‚é–“ï¼š{w.get('time', 'N/A')}<br>
                            {extra_meta}
                            ğŸ”‘ é—œéµå­—ï¼š{kw_str}<br>
                            ğŸ”— <a href="{w.get('link', '#')}">æŸ¥çœ‹è©³æƒ…</a>
                        </div>
                        {details_html}
                        {coord_html}
                    </div>
                """
            return result

        if today_warnings:
            html += f"<h2>ğŸ†• ä»Šæ—¥æ–°å¢èˆªè¡Œè­¦å‘Š ({len(today_warnings)} ç­†)</h2>"
            html += _render_warnings(today_warnings, "today", "ä»Šæ—¥")
        if history_warnings:
            html += f"<h2>ğŸ“š éå¾€èˆªè¡Œè­¦å‘Š ({len(history_warnings)} ç­†)</h2>"
            html += _render_warnings(history_warnings, "history", "æ­·å²")

        html += """
            <div class="footer">
                <p>æ­¤ç‚ºè‡ªå‹•ç™¼é€çš„éƒµä»¶ï¼Œè«‹å‹¿ç›´æ¥å›è¦†</p>
                <p>èˆªè¡Œè­¦å‘Šç›£æ§ç³»çµ± v3.1 | Navigation Warning Monitor System</p>
            </div></div></body></html>
        """
        return html


# ==================== 5. UKMTO çˆ¬èŸ² (v3.1 - åº§æ¨™å¾ __NEXT_DATA__ è®€å–) ====================
class UKMTOScraper:
    """
    çˆ¬å– UKMTO èˆªè¡Œè­¦å‘Š
    åº§æ¨™å„ªå…ˆé †åºï¼š
      1. __NEXT_DATA__ JSONï¼ˆç²¾ç¢ºï¼Œä¾†è‡ªåœ°åœ– Pin åŸå§‹è³‡æ–™ï¼‰
      2. _next/data/ APIï¼ˆå‚™ç”¨ï¼Œä¸éœ€è¦ JSï¼‰
      3. æ–‡å­—è§£æ fallbackï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰
    """

    URL = "https://www.ukmto.org/recent-incidents"

    MONTH_MAP = {
        "January": 1, "February": 2, "March": 3,    "April": 4,
        "May": 5,     "June": 6,     "July": 7,      "August": 8,
        "September": 9, "October": 10, "November": 11, "December": 12,
    }

    def __init__(self, db_manager, keyword_manager, teams_notifier, coord_extractor, days=30):
        self.db_manager       = db_manager
        self.keyword_manager  = keyword_manager
        self.keywords         = keyword_manager.get_keywords()
        self.teams_notifier   = teams_notifier
        self.coord_extractor  = coord_extractor
        self.days             = days

        now = datetime.now(tz=timezone.utc)
        self.cutoff_date  = now - timedelta(days=days)
        self.today_start  = now.replace(hour=0, minute=0, second=0, microsecond=0)

        self.new_warnings_today        = []
        self.new_warnings_history      = []
        self.captured_warnings_today   = []
        self.captured_warnings_history = []

        # ç”¨ä¾†å¿«å–å¾ __NEXT_DATA__ è§£æå‡ºçš„åº§æ¨™ dict
        # key = incident id (str)ï¼Œvalue = (lat, lon)
        self._next_data_coords: dict = {}

        print(f"  ğŸ‡¬ğŸ‡§ UKMTO çˆ¬èŸ²è¨­å®š:")
        print(f"     - æŠ“å–ç¯„åœ: æœ€è¿‘ {days} å¤© (å¾ {self.cutoff_date.strftime('%Y-%m-%d')} èµ·)")
        print(f"     - ä»Šæ—¥å®šç¾©: {self.today_start.strftime('%Y-%m-%d')} 00:00 UTC èµ·")
        print(f"     - åº§æ¨™ç­–ç•¥: __NEXT_DATA__ â†’ _next/data API â†’ æ–‡å­—è§£æ")

        print("  ğŸŒ æ­£åœ¨å•Ÿå‹• Chrome WebDriver (UKMTO)...")
        self.driver = self._init_driver()
        self.wait   = WebDriverWait(self.driver, 20)
        print("  âœ… WebDriver å•Ÿå‹•æˆåŠŸ (UKMTO)")

    # ------------------------------------------------------------------
    # WebDriver
    # ------------------------------------------------------------------
    def _init_driver(self) -> webdriver.Chrome:
        options = webdriver.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--ignore-certificate-errors")
        options.add_argument("--ignore-ssl-errors=yes")
        options.add_argument("--allow-insecure-localhost")
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        options.add_experimental_option('excludeSwitches', ['enable-logging', 'enable-automation'])
        options.add_experimental_option('useAutomationExtension', False)

        driver_path = self._find_chromedriver()
        service = Service(executable_path=driver_path) if driver_path else Service()
        if platform.system() == 'Windows':
            service.creation_flags = subprocess.CREATE_NO_WINDOW

        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(60)
        return driver

    def _find_chromedriver(self) -> str | None:
        env_path = os.environ.get("CHROMEDRIVER_PATH")
        if env_path and os.path.exists(env_path):
            return env_path
        common_paths = [
            r"C:\chromedriver\chromedriver.exe",
            r"C:\Program Files\Google\Chrome\Application\chromedriver.exe",
            os.path.join(os.environ.get("LOCALAPPDATA", ""), "Google", "Chrome", "Application", "chromedriver.exe"),
            os.path.join(os.environ.get("USERPROFILE", ""), "chromedriver.exe"),
            os.path.join(os.environ.get("USERPROFILE", ""), "Downloads", "chromedriver.exe"),
            "chromedriver.exe", "chromedriver",
        ]
        for p in common_paths:
            if p and os.path.exists(p):
                return p
        try:
            return ChromeDriverManager().install()
        except Exception as e:
            print(f"  âš ï¸  webdriver_manager å¤±æ•—: {e}")
        return None

    # ------------------------------------------------------------------
    # â˜… æ ¸å¿ƒæ–°å¢ï¼šå¾ __NEXT_DATA__ æå–æ‰€æœ‰äº‹ä»¶åº§æ¨™
    # ------------------------------------------------------------------
    def _extract_coords_from_next_data(self) -> dict:
        """
        å¾é é¢çš„ <script id="__NEXT_DATA__"> æå–æ‰€æœ‰äº‹ä»¶çš„ç²¾ç¢ºåº§æ¨™ã€‚

        UKMTO Next.js è³‡æ–™çµæ§‹ï¼ˆå¸¸è¦‹è·¯å¾‘ï¼Œä¾å„ªå…ˆé †åºå˜—è©¦ï¼‰ï¼š
          props.pageProps.incidents[].latitude / .longitude
          props.pageProps.data.incidents[].lat / .lng
          props.pageProps.initialData[].position.lat / .lng

        å›å‚³: { incident_id_str: (lat, lon), ... }
        """
        coord_map = {}
        try:
            script_el = self.driver.find_element(By.ID, "__NEXT_DATA__")
            raw       = script_el.get_attribute("innerHTML")
            data      = json.loads(raw)
            print("  âœ… æˆåŠŸè®€å– __NEXT_DATA__")

            # â”€â”€ å˜—è©¦å¤šç¨®å·²çŸ¥è·¯å¾‘ â”€â”€
            page_props = data.get("props", {}).get("pageProps", {})

            # è·¯å¾‘å€™é¸æ¸…å–®ï¼ˆ(incidents_list, id_key, lat_key, lon_key)ï¼‰
            candidates = [
                # è·¯å¾‘ Aï¼šç›´æ¥ incidents é™£åˆ—
                (page_props.get("incidents", []),   "id",  "latitude",  "longitude"),
                (page_props.get("incidents", []),   "id",  "lat",       "lng"),
                (page_props.get("incidents", []),   "_id", "latitude",  "longitude"),
                # è·¯å¾‘ Bï¼šdata.incidents
                (page_props.get("data", {}).get("incidents", []), "id",  "latitude",  "longitude"),
                (page_props.get("data", {}).get("incidents", []), "id",  "lat",       "lng"),
                # è·¯å¾‘ Cï¼šinitialData
                (page_props.get("initialData", []), "id",  "latitude",  "longitude"),
                (page_props.get("initialData", []), "id",  "lat",       "lng"),
            ]

            for incidents, id_key, lat_key, lon_key in candidates:
                if not incidents:
                    continue
                for inc in incidents:
                    try:
                        inc_id = str(inc.get(id_key, ""))
                        lat    = inc.get(lat_key)
                        lon    = inc.get(lon_key)

                        # æœ‰äº›è³‡æ–™æŠŠåº§æ¨™åŒ…åœ¨ position / location å­ç‰©ä»¶å…§
                        if lat is None or lon is None:
                            pos = inc.get("position") or inc.get("location") or inc.get("coordinates") or {}
                            if isinstance(pos, dict):
                                lat = pos.get("lat") or pos.get("latitude")
                                lon = pos.get("lng") or pos.get("lon") or pos.get("longitude")
                            elif isinstance(pos, (list, tuple)) and len(pos) >= 2:
                                lat, lon = pos[0], pos[1]

                        if lat is not None and lon is not None:
                            lat_f = float(lat)
                            lon_f = float(lon)
                            if -90 <= lat_f <= 90 and -180 <= lon_f <= 180:
                                coord_map[inc_id] = (lat_f, lon_f)
                    except (ValueError, TypeError):
                        continue

                if coord_map:
                    print(f"  ğŸ“¡ __NEXT_DATA__ å…±è§£æåˆ° {len(coord_map)} ç­†åº§æ¨™")
                    return coord_map

            # â”€â”€ è‹¥æ¨™æº–è·¯å¾‘éƒ½æ²’æ‰¾åˆ°ï¼Œåšéè¿´æœå°‹ï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰â”€â”€
            if not coord_map:
                print("  âš ï¸  æ¨™æº–è·¯å¾‘æœªæ‰¾åˆ°åº§æ¨™ï¼Œå˜—è©¦éè¿´æœå°‹ __NEXT_DATA__...")
                coord_map = self._deep_search_coords(data)
                if coord_map:
                    print(f"  ğŸ“¡ éè¿´æœå°‹å…±æ‰¾åˆ° {len(coord_map)} ç­†åº§æ¨™")

        except Exception as e:
            print(f"  âš ï¸  __NEXT_DATA__ è§£æå¤±æ•—: {e}")

        return coord_map

    def _deep_search_coords(self, obj, depth=0, result=None) -> dict:
        """
        éè¿´æœå°‹ JSON ç‰©ä»¶ä¸­æ‰€æœ‰å« lat/lon çš„ç¯€é»ã€‚
        æœ€å¤šéè¿´ 6 å±¤ï¼Œé¿å…æ•ˆèƒ½å•é¡Œã€‚
        """
        if result is None:
            result = {}
        if depth > 6:
            return result

        if isinstance(obj, dict):
            lat = obj.get("latitude") or obj.get("lat")
            lon = obj.get("longitude") or obj.get("lng") or obj.get("lon")
            if lat is not None and lon is not None:
                try:
                    lat_f, lon_f = float(lat), float(lon)
                    if -90 <= lat_f <= 90 and -180 <= lon_f <= 180:
                        inc_id = str(obj.get("id") or obj.get("_id") or len(result))
                        result[inc_id] = (lat_f, lon_f)
                except (ValueError, TypeError):
                    pass
            for v in obj.values():
                self._deep_search_coords(v, depth + 1, result)

        elif isinstance(obj, list):
            for item in obj:
                self._deep_search_coords(item, depth + 1, result)

        return result

    # ------------------------------------------------------------------
    # â˜… å‚™ç”¨æ–¹æ¡ˆï¼š_next/data/ APIï¼ˆä¸éœ€è¦ JS åŸ·è¡Œï¼‰
    # ------------------------------------------------------------------
    def _fetch_coords_from_next_api(self) -> dict:
        """
        å˜—è©¦é€é Next.js çš„ _next/data/{buildId}/recent-incidents.json ç«¯é»å–å¾—åº§æ¨™ã€‚
        buildId å¾å·²è¼‰å…¥é é¢çš„ __NEXT_DATA__ ä¸­è®€å–ã€‚
        """
        coord_map = {}
        try:
            script_el = self.driver.find_element(By.ID, "__NEXT_DATA__")
            raw       = json.loads(script_el.get_attribute("innerHTML"))
            build_id  = raw.get("buildId", "")
            if not build_id:
                return coord_map

            api_url  = f"https://www.ukmto.org/_next/data/{build_id}/recent-incidents.json"
            print(f"  ğŸ”„ å˜—è©¦ _next/data API: {api_url}")
            resp = requests.get(api_url, timeout=15, verify=False,
                                headers={"User-Agent": "Mozilla/5.0"})
            if resp.status_code == 200:
                api_data   = resp.json()
                page_props = api_data.get("pageProps", {})
                incidents  = (
                    page_props.get("incidents") or
                    page_props.get("data", {}).get("incidents") or
                    []
                )
                for inc in incidents:
                    try:
                        inc_id = str(inc.get("id") or inc.get("_id", ""))
                        lat    = inc.get("latitude") or inc.get("lat")
                        lon    = inc.get("longitude") or inc.get("lng") or inc.get("lon")
                        if lat is not None and lon is not None:
                            coord_map[inc_id] = (float(lat), float(lon))
                    except (ValueError, TypeError):
                        continue
                if coord_map:
                    print(f"  âœ… _next/data API å–å¾— {len(coord_map)} ç­†åº§æ¨™")
        except Exception as e:
            print(f"  âš ï¸  _next/data API å¤±æ•—: {e}")
        return coord_map

    # ------------------------------------------------------------------
    # æ—¥æœŸè§£æ
    # ------------------------------------------------------------------
    def _parse_date(self, date_str: str) -> datetime | None:
        parts = date_str.strip().split()
        if len(parts) != 3:
            return None
        try:
            day   = int(parts[0])
            month = self.MONTH_MAP.get(parts[1])
            year  = int(parts[2])
            if not month:
                return None
            return datetime(year, month, day, tzinfo=timezone.utc)
        except (ValueError, TypeError):
            return None

    # ------------------------------------------------------------------
    # ä¸»è¦çˆ¬å–æµç¨‹
    # ------------------------------------------------------------------
    def scrape(self):
        print(f"\nğŸ‡¬ğŸ‡§ é–‹å§‹çˆ¬å– UKMTO èˆªè¡Œè­¦å‘Š...")
        print(f"  ğŸŒ ç›®æ¨™ç¶²å€: {self.URL}")

        try:
            self.driver.get(self.URL)
            self.wait.until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, "ul.IncidentList_incidentList__NGsl0")
                )
            )
            print("  âœ… é é¢è¼‰å…¥å®Œæˆ")
            time.sleep(2)

            # â”€â”€ Step 1: å¾ __NEXT_DATA__ æ‰¹æ¬¡å–å¾—æ‰€æœ‰åº§æ¨™ â”€â”€
            print("\n  ğŸ“¡ Step 1: å¾ __NEXT_DATA__ æå–åº§æ¨™...")
            self._next_data_coords = self._extract_coords_from_next_data()

            # â”€â”€ Step 2: è‹¥ __NEXT_DATA__ æ²’æœ‰çµæœï¼Œå˜—è©¦ API â”€â”€
            if not self._next_data_coords:
                print("  ğŸ”„ Step 2: __NEXT_DATA__ ç„¡çµæœï¼Œå˜—è©¦ _next/data API...")
                self._next_data_coords = self._fetch_coords_from_next_api()

            if self._next_data_coords:
                print(f"  âœ… åº§æ¨™é è¼‰å®Œæˆï¼Œå…± {len(self._next_data_coords)} ç­†")
            else:
                print("  âš ï¸  ç„¡æ³•é è¼‰åº§æ¨™ï¼Œå°‡æ”¹ç”¨æ–‡å­—è§£æ fallback")

            # â”€â”€ Step 3: é€ç­†è™•ç†äº‹ä»¶åˆ—è¡¨ â”€â”€
            print("\n  ğŸ“‹ Step 3: é–‹å§‹è§£æäº‹ä»¶åˆ—è¡¨...")
            li_elements = self.driver.find_elements(
                By.CSS_SELECTOR,
                "ul.IncidentList_incidentList__NGsl0 > li.IncidentList_incident__HgGtN"
            )
            print(f"  å…±æ‰¾åˆ° {len(li_elements)} ç­†äº‹ä»¶ï¼Œç¯©é¸æœ€è¿‘ {self.days} å¤©...")

            for elem in li_elements:
                try:
                    self._process_incident(elem)
                except StopIteration as si:
                    print(f"  â­ï¸  {si}")
                    break
                except Exception as e:
                    print(f"  âš ï¸  è™•ç†äº‹ä»¶æ™‚å‡ºéŒ¯: {e}")
                    continue

        except Exception as e:
            print(f"  âŒ UKMTO çˆ¬å–éŒ¯èª¤: {e}")
            traceback.print_exc()
        finally:
            try:
                self.driver.quit()
                print("  ğŸ”’ WebDriver å·²é—œé–‰ (UKMTO)")
            except:
                pass

        total_new = len(self.new_warnings_today) + len(self.new_warnings_history)
        print(f"\nğŸ‡¬ğŸ‡§ UKMTO çˆ¬å–å®Œæˆ:")
        print(f"   ğŸ†• ä»Šæ—¥æ–°å¢: {len(self.new_warnings_today)} ç­†")
        print(f"   ğŸ“š æ­·å²è³‡æ–™: {len(self.new_warnings_history)} ç­†")
        print(f"   ğŸ“Š ç¸½è¨ˆ: {total_new} ç­†")

        return {'today': self.new_warnings_today, 'history': self.new_warnings_history}

    # ------------------------------------------------------------------
    # è™•ç†å–®ä¸€äº‹ä»¶
    # ------------------------------------------------------------------
    def _process_incident(self, elem):
        # â”€â”€ åŸºæœ¬æ¬„ä½ â”€â”€
        incident_id = elem.get_attribute("id") or ""

        try:
            title = elem.find_element(By.CSS_SELECTOR, "h3.IncidentList_title__cOmOY button").text.strip()
        except Exception:
            title = "N/A"

        try:
            colour = elem.find_element(By.CSS_SELECTOR, "span.Pin_pin__dpf_F").get_attribute("data-colour") or "N/A"
        except Exception:
            colour = "N/A"

        try:
            date_str      = elem.find_element(By.CSS_SELECTOR, "ul.IncidentList_meta__JmhSj li span").text.strip()
            incident_date = self._parse_date(date_str)
        except Exception:
            date_str      = "N/A"
            incident_date = None

        try:
            details = elem.find_element(By.CSS_SELECTOR, "p.IncidentList_details__bwUAz").text.strip()
        except Exception:
            details = "N/A"

        # â”€â”€ æ—¥æœŸç¯©é¸ â”€â”€
        if incident_date is None:
            print(f"  âš ï¸  è·³éï¼ˆæ—¥æœŸç„¡æ³•è§£æï¼‰ï¼š{title}")
            return
        if incident_date < self.cutoff_date:
            raise StopIteration(f"è¶…å‡ºç¯„åœï¼Œåœæ­¢ï¼ˆ{date_str}ï¼‰")

        is_today    = incident_date >= self.today_start
        time_label  = "ğŸ†• ä»Šæ—¥" if is_today else "ğŸ“š æ­·å²"
        colour_icon = "ğŸ”´" if colour == "Red" else "ğŸŸ¡"
        print(f"  {time_label} {colour_icon} [{date_str}] {title}")

        # â”€â”€ â˜… åº§æ¨™å–å¾—ï¼ˆä¸‰å±¤å„ªå…ˆé †åºï¼‰â”€â”€
        coordinates  = []
        coord_source = "none"

        # å„ªå…ˆ 1ï¼šå¾é è¼‰çš„ __NEXT_DATA__ / API åº§æ¨™ dict æŸ¥è©¢
        if incident_id and incident_id in self._next_data_coords:
            coordinates  = [self._next_data_coords[incident_id]]
            coord_source = "next_data"
            lat, lon     = coordinates[0]
            lat_dir      = 'N' if lat >= 0 else 'S'
            lon_dir      = 'E' if lon >= 0 else 'W'
            print(f"    ğŸ“¡ __NEXT_DATA__ åº§æ¨™: {abs(lat):.4f}Â°{lat_dir}, {abs(lon):.4f}Â°{lon_dir}")

        # å„ªå…ˆ 2ï¼šè‹¥ id å°ä¸ä¸Šï¼Œå˜—è©¦ç”¨æ¨™é¡Œæ¨¡ç³Šæ¯”å°ï¼ˆUKMTO id æœ‰æ™‚å¸¶ # å‰ç¶´ï¼‰
        if not coordinates and self._next_data_coords:
            clean_id = incident_id.lstrip('#').strip()
            for key, coord in self._next_data_coords.items():
                if clean_id and (clean_id in key or key in clean_id):
                    coordinates  = [coord]
                    coord_source = "next_data"
                    print(f"    ğŸ“¡ __NEXT_DATA__ æ¨¡ç³Šæ¯”å°åº§æ¨™ (id={key})")
                    break

        # å„ªå…ˆ 3ï¼šæ–‡å­—è§£æ fallback
        if not coordinates:
            text_coords = self.coord_extractor.extract_coordinates(details)
            if text_coords:
                coordinates  = text_coords
                coord_source = "text"
                print(f"    ğŸ“ æ–‡å­—è§£æåº§æ¨™: {len(coordinates)} å€‹")
            else:
                print(f"    â„¹ï¸  ç„¡åº§æ¨™è³‡è¨Š")

        # â”€â”€ é—œéµå­—æ¯”å° â”€â”€
        matched_keywords = [k for k in self.keywords if k.lower() in (title + " " + details).lower()]
        if not matched_keywords:
            matched_keywords = ["UKMTO"]

        # â”€â”€ å­˜å…¥è³‡æ–™åº« â”€â”€
        db_data = (
            "UKMTO", title, self.URL, date_str,
            ', '.join(matched_keywords),
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            coordinates
        )
        is_new, w_id = self.db_manager.save_warning(db_data, source_type="UKMTO")

        if is_new and w_id:
            warning_data = {
                'id':           w_id,
                'bureau':       "UKMTO",
                'title':        title,
                'link':         self.URL,
                'time':         date_str,
                'keywords':     matched_keywords,
                'source':       'UKMTO',
                'colour':       colour,
                'coordinates':  coordinates,
                'coord_source': coord_source,   # â† æ–°å¢ï¼šè®“ Email é¡¯ç¤ºåº§æ¨™ä¾†æºæ¨™ç±¤
                'details':      details,          # â† æ–°å¢é€™è¡Œ
            }
            if is_today:
                self.new_warnings_today.append(w_id)
                self.captured_warnings_today.append(warning_data)
                print(f"    ğŸ’¾ æ–°è³‡æ–™å·²å­˜å…¥ [ä»Šæ—¥] (ID: {w_id})")
            else:
                self.new_warnings_history.append(w_id)
                self.captured_warnings_history.append(warning_data)
                print(f"    ğŸ’¾ æ–°è³‡æ–™å·²å­˜å…¥ [æ­·å²] (ID: {w_id})")
        else:
            print(f"    â„¹ï¸  è³‡æ–™å·²å­˜åœ¨")


# ==================== 6. å°ç£èˆªæ¸¯å±€çˆ¬èŸ² ====================
class TWMaritimePortBureauScraper:
    def __init__(self, db_manager, keyword_manager, teams_notifier, coord_extractor, days=3):
        self.db_manager      = db_manager
        self.keyword_manager = keyword_manager
        self.keywords        = keyword_manager.get_keywords()
        self.teams_notifier  = teams_notifier
        self.coord_extractor = coord_extractor
        self.base_url        = "https://www.motcmpb.gov.tw/Information/Notice?SiteId=1&NodeId=483"
        self.days            = days
        self.cutoff_date     = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=days)
        self.today_start     = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

        self.new_warnings_today        = []
        self.new_warnings_history      = []
        self.captured_warnings_today   = []
        self.captured_warnings_history = []
        self.target_categories         = {'333': 'ç¤™èˆªå…¬å‘Š', '334': 'å°„æ“Šå…¬å‘Š'}

        print(f"  ğŸ“… å°ç£èˆªæ¸¯å±€çˆ¬èŸ²è¨­å®š: æœ€è¿‘ {days} å¤© | ä»Šæ—¥: {self.today_start.strftime('%Y-%m-%d')}")
        print("  ğŸŒ æ­£åœ¨å•Ÿå‹• Chrome WebDriver (å°ç£èˆªæ¸¯å±€)...")

        options = webdriver.ChromeOptions()
        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_experimental_option('prefs', {'profile.default_content_setting_values.notifications': 2})
        options.add_experimental_option('excludeSwitches', ['enable-logging', 'enable-automation'])
        options.add_experimental_option('useAutomationExtension', False)

        try:
            service = Service(ChromeDriverManager().install())
            if platform.system() == 'Windows':
                service.creation_flags = subprocess.CREATE_NO_WINDOW
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(60)
            self.wait = WebDriverWait(self.driver, 20)
            print("  âœ… WebDriver å•Ÿå‹•æˆåŠŸ (å°ç£èˆªæ¸¯å±€)")
        except Exception as e:
            print(f"  âŒ WebDriver å•Ÿå‹•å¤±æ•—: {e}"); raise

    def check_keywords(self, text):
        if not text: return []
        matched = [k for k in self.keywords if k.lower() in text.lower()]
        for kw in ['ç¤™èˆª', 'å°„æ“Š']:
            if kw in text and kw not in matched:
                matched.append(kw)
        return matched

    def parse_date(self, date_string):
        try:
            m = re.match(r'^(\d{2,4})[/-](\d{1,2})[/-](\d{1,2})$', date_string.strip())
            if m:
                y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))
                if y < 1000: y += 1911
                return datetime(y, mo, d)
        except Exception:
            pass
        return None

    def is_within_date_range(self, date_string):
        if not date_string: return None, False
        pd = self.parse_date(date_string)
        if pd:
            if pd < self.cutoff_date: return None, False
            return pd, pd >= self.today_start
        return None, False

    def click_category_tab(self, category_id):
        try:
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.tabs a")))
            tab_xpath = f"//div[@class='tabs']//a[@data-val='{category_id}']" if category_id else "//div[@class='tabs']//a[@class='active']"
            tab = self.wait.until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))
            self.driver.execute_script("arguments[0].scrollIntoView(true);", tab)
            time.sleep(0.5)
            self.driver.execute_script("arguments[0].click();", tab)
            time.sleep(3)
            return True
        except Exception as e:
            print(f"    âš ï¸ é»æ“Šåˆ†é¡æ¨™ç±¤å¤±æ•—: {e}"); return False

    def get_notices_selenium(self, page=1, base_category_id=None):
        try:
            category_name = self.target_categories.get(base_category_id, 'å…¨éƒ¨') if base_category_id else 'å…¨éƒ¨'
            print(f"  æ­£åœ¨è«‹æ±‚å°ç£èˆªæ¸¯å±€ [{category_name}] ç¬¬ {page} é ...")

            if page == 1:
                self.driver.get(self.base_url)
                time.sleep(3)
                if base_category_id and not self.click_category_tab(base_category_id):
                    return {'has_data': False, 'notices': [], 'processed': 0}
            else:
                try:
                    nb = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "li.next a")))
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", nb)
                    time.sleep(0.5)
                    self.driver.execute_script("arguments[0].click();", nb)
                    time.sleep(3)
                except Exception as e:
                    print(f"    âš ï¸ ç„¡æ³•ç¿»é : {e}"); return {'has_data': False, 'notices': [], 'processed': 0}

            try:
                self.wait.until(EC.presence_of_element_located((By.ID, "table")))
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#table dl")))
            except Exception as e:
                print(f"    âš ï¸ ç­‰å¾…å…§å®¹è¼‰å…¥è¶…æ™‚: {e}"); return {'has_data': False, 'notices': [], 'processed': 0}

            soup         = BeautifulSoup(self.driver.page_source, 'html.parser')
            table_div    = soup.find('div', id='table')
            if not table_div: return {'has_data': False, 'notices': [], 'processed': 0}
            contents_div = table_div.find('div', class_='contents')
            if not contents_div: return {'has_data': False, 'notices': [], 'processed': 0}
            data_dl_list = [dl for dl in contents_div.find_all('dl') if 'con-title' not in dl.get('class', [])]
            print(f"    ğŸ“‹ æ‰¾åˆ° {len(data_dl_list)} å€‹è³‡æ–™åˆ—")
            if not data_dl_list: return {'has_data': False, 'notices': [], 'processed': 0}

            processed_count = 0
            for idx, dl in enumerate(data_dl_list, 1):
                try:
                    dt_list = dl.find_all('dt')
                    dd = dl.find('dd')
                    if len(dt_list) < 2 or not dd: continue
                    processed_count += 1
                    date = dt_list[1].get_text(strip=True)
                    unit = dt_list[2].get_text(strip=True) if len(dt_list) > 2 else 'å°ç£èˆªæ¸¯å±€'
                    link_tag = dd.find('a')
                    if link_tag:
                        title = link_tag.get_text(strip=True)
                        link  = link_tag.get('href', '')
                        if link and not link.startswith('http'):
                            link = f"https://www.motcmpb.gov.tw{link}" if link.startswith('/') else f"https://www.motcmpb.gov.tw/{link}"
                    else:
                        title = dd.get_text(strip=True); link = ''

                    parsed_date, is_today = self.is_within_date_range(date)
                    if parsed_date is None: continue

                    matched_keywords = self.check_keywords(title)
                    if not matched_keywords: continue

                    coordinates = []
                    title_coords = self.coord_extractor.extract_coordinates(title)
                    if title_coords: coordinates.extend(title_coords)

                    if link:
                        try:
                            self.driver.execute_script("window.open('');")
                            self.driver.switch_to.window(self.driver.window_handles[1])
                            self.driver.set_page_load_timeout(10)
                            self.driver.get(link); time.sleep(2)
                            detail_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                            content_div = (
                                detail_soup.find('div', class_='editor_Content') or
                                detail_soup.find('div', class_='content') or
                                detail_soup.find('div', id='content') or
                                detail_soup.find('article') or
                                detail_soup.find('div', id='container')
                            )
                            if content_div:
                                for pc in self.coord_extractor.extract_coordinates(content_div.get_text()):
                                    if pc not in coordinates:
                                        coordinates.append(pc)
                            self.driver.close()
                            self.driver.switch_to.window(self.driver.window_handles[0])
                            self.driver.set_page_load_timeout(60)
                            time.sleep(1)
                        except Exception as e:
                            print(f"          âš ï¸ ç„¡æ³•å¾ç¶²é æå–åº§æ¨™: {e}")
                            try:
                                if len(self.driver.window_handles) > 1:
                                    self.driver.close()
                                    self.driver.switch_to.window(self.driver.window_handles[0])
                                    self.driver.set_page_load_timeout(60)
                            except:
                                pass

                    db_data = (unit, title, link, date, ', '.join(matched_keywords), datetime.now().strftime('%Y-%m-%d %H:%M:%S'), coordinates)
                    is_new, w_id = self.db_manager.save_warning(db_data, source_type="TW_MPB")

                    if is_new and w_id:
                        warning_data = {
                            'id': w_id, 'bureau': unit, 'title': title, 'link': link,
                            'time': date, 'keywords': matched_keywords,
                            'source': 'TW_MPB', 'category': category_name,
                            'coordinates': coordinates, 'coord_source': 'text'
                        }
                        if is_today:
                            self.new_warnings_today.append(w_id)
                            self.captured_warnings_today.append(warning_data)
                            print(f"        ğŸ’¾ æ–°è³‡æ–™å·²å­˜å…¥ [ä»Šæ—¥] (ID: {w_id})")
                        else:
                            self.new_warnings_history.append(w_id)
                            self.captured_warnings_history.append(warning_data)
                            print(f"        ğŸ’¾ æ–°è³‡æ–™å·²å­˜å…¥ [æ­·å²] (ID: {w_id})")
                    else:
                        print(f"        â„¹ï¸ è³‡æ–™å·²å­˜åœ¨")

                except Exception as e:
                    print(f"    âš ï¸ è™•ç†é …ç›® {idx} æ™‚å‡ºéŒ¯: {e}")
                    traceback.print_exc()
                    continue

            return {'has_data': processed_count > 0, 'notices': [], 'processed': processed_count}

        except Exception as e:
            print(f"  âŒ è«‹æ±‚å¤±æ•—: {e}")
            traceback.print_exc()
            return {'has_data': False, 'notices': [], 'processed': 0}

    def scrape_all_pages(self, max_pages=5):
        print(f"\nğŸ‡¹ğŸ‡¼ é–‹å§‹çˆ¬å–å°ç£èˆªæ¸¯å±€èˆªè¡Œè­¦å‘Š...")
        print(f"  ğŸŒ ç›®æ¨™ç¶²å€: {self.base_url}")
        try:
            for category_id, category_name in self.target_categories.items():
                print(f"\n  ğŸ“‹ çˆ¬å–åˆ†é¡: {category_name} (ID: {category_id})")
                for page in range(1, max_pages + 1):
                    result = self.get_notices_selenium(page, category_id)
                    if not result['has_data']:
                        print(f"    ğŸ›‘ ç¬¬ {page} é æ²’æœ‰è³‡æ–™ï¼Œåœæ­¢")
                        break
                    if result['processed'] < 10:
                        print(f"    â„¹ï¸ ç¬¬ {page} é è³‡æ–™è¼ƒå°‘ï¼Œå¯èƒ½å·²æ¥è¿‘æœ€å¾Œä¸€é ")
                    time.sleep(2)
        except Exception as e:
            print(f"âŒ å°ç£èˆªæ¸¯å±€çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
        finally:
            try:
                self.driver.quit()
                print("  ğŸ”’ WebDriver å·²é—œé–‰ (å°ç£èˆªæ¸¯å±€)")
            except:
                pass

        total_new = len(self.new_warnings_today) + len(self.new_warnings_history)
        print(f"\nğŸ‡¹ğŸ‡¼ å°ç£èˆªæ¸¯å±€çˆ¬å–å®Œæˆ:")
        print(f"   ğŸ†• ä»Šæ—¥æ–°å¢: {len(self.new_warnings_today)} ç­†")
        print(f"   ğŸ“š æ­·å²è³‡æ–™: {len(self.new_warnings_history)} ç­†")
        print(f"   ğŸ“Š ç¸½è¨ˆ: {total_new} ç­†")
        return {'today': self.new_warnings_today, 'history': self.new_warnings_history}


# ==================== 7. ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ² ====================
class CNMSANavigationWarningsScraper:
    def __init__(self, db_manager, keyword_manager, teams_notifier, coord_extractor, headless=True, days=3):
        self.db_manager      = db_manager
        self.keyword_manager = keyword_manager
        self.keywords        = keyword_manager.get_keywords()
        self.teams_notifier  = teams_notifier
        self.coord_extractor = coord_extractor

        print("ğŸ‡¨ğŸ‡³ åˆå§‹åŒ–ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ²...")

        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        options.add_experimental_option('prefs', {'profile.managed_default_content_settings.images': 2})
        options.add_experimental_option('excludeSwitches', ['enable-logging'])

        try:
            service = Service(ChromeDriverManager().install())
            if platform.system() == 'Windows':
                service.creation_flags = subprocess.CREATE_NO_WINDOW
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(120)
            self.wait = WebDriverWait(self.driver, 15)
            print("  âœ… WebDriver å•Ÿå‹•æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ WebDriver å•Ÿå‹•å¤±æ•—: {e}"); raise

        self.days        = days
        self.cutoff_date = datetime.now() - timedelta(days=days)
        self.today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

        self.new_warnings_today        = []
        self.new_warnings_history      = []
        self.captured_warnings_today   = []
        self.captured_warnings_history = []

        print(f"  ğŸ“… ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ²è¨­å®š: æœ€è¿‘ {days} å¤© | ä»Šæ—¥: {self.today_start.strftime('%Y-%m-%d')}")

    def check_keywords(self, text):
        return [k for k in self.keywords if k.lower() in text.lower()]

    def parse_date(self, date_str):
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except:
                continue
        return None

    def scrape_bureau_warnings(self, bureau_name, bureau_element):
        print(f"  ğŸ” æŠ“å–: {bureau_name}")
        max_retries = 3
        for retry in range(max_retries):
            try:
                if retry > 0:
                    print(f"    ğŸ”„ é‡è©¦ç¬¬ {retry} æ¬¡...")
                    try:
                        bureau_element = self.driver.find_element(
                            By.XPATH,
                            f"//div[@class='nav_lv2_text' and contains(text(), '{bureau_name}')]"
                        )
                    except:
                        print(f"    âš ï¸ ç„¡æ³•é‡æ–°ç²å–å…ƒç´ : {bureau_name}"); break

                self.driver.execute_script("arguments[0].scrollIntoView(true); arguments[0].click();", bureau_element)
                time.sleep(2)
                self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "right_main")))

                processed_count = 0
                max_items       = 100

                while processed_count < max_items:
                    try:
                        items = self.driver.find_elements(By.CSS_SELECTOR, ".right_main a")
                        if processed_count >= len(items):
                            break
                        item = items[processed_count]

                        try:
                            title = item.get_attribute('title') or item.text.strip()
                            title = re.sub(r'\s*\d{4}-\d{2}-\d{2}\s*$', '', title)
                            if not title:
                                processed_count += 1; continue

                            matched = self.check_keywords(title)
                            if not matched:
                                processed_count += 1; continue

                            link = item.get_attribute('href') or ''
                            if link.startswith('/'):
                                link = f"https://www.msa.gov.cn{link}"

                            try:
                                publish_time = item.find_element(By.CSS_SELECTOR, ".time").text.strip()
                            except:
                                m = re.search(r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}', item.text)
                                publish_time = m.group() if m else ""

                            if publish_time:
                                p_date = self.parse_date(publish_time)
                                if p_date:
                                    if p_date < self.cutoff_date:
                                        processed_count += 1; continue
                                    is_today   = p_date >= self.today_start
                                    time_label = "ğŸ†• ä»Šæ—¥" if is_today else "ğŸ“š æ­·å²"
                                    print(f"      {time_label} è³‡æ–™: {publish_time}")
                                else:
                                    processed_count += 1; continue
                            else:
                                processed_count += 1; continue

                            coordinates = []
                            title_coords = self.coord_extractor.extract_coordinates(title)
                            if title_coords:
                                coordinates.extend(title_coords)

                            if link and not link.startswith('javascript'):
                                try:
                                    self.driver.execute_script("window.open('');")
                                    self.driver.switch_to.window(self.driver.window_handles[-1])
                                    self.driver.set_page_load_timeout(10)
                                    try:
                                        self.driver.get(link); time.sleep(1)
                                        page_coords = self.coord_extractor.extract_from_html(self.driver.page_source)
                                        for pc in page_coords:
                                            if pc not in coordinates:
                                                coordinates.append(pc)
                                    except Exception as e:
                                        print(f"      âš ï¸ é é¢è¼‰å…¥å¤±æ•—: {e}")
                                    finally:
                                        try:
                                            self.driver.close()
                                            self.driver.switch_to.window(self.driver.window_handles[0])
                                            self.driver.set_page_load_timeout(120)
                                        except:
                                            pass
                                except Exception as e:
                                    print(f"      âš ï¸ ç„¡æ³•å¾ç¶²é æå–åº§æ¨™: {e}")

                            db_data = (
                                bureau_name, title, link, publish_time,
                                ', '.join(matched),
                                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                coordinates
                            )
                            is_new, w_id = self.db_manager.save_warning(db_data, source_type="CN_MSA")

                            if is_new and w_id:
                                warning_data = {
                                    'id': w_id, 'bureau': bureau_name, 'title': title,
                                    'link': link, 'time': publish_time, 'keywords': matched,
                                    'source': 'CN_MSA', 'coordinates': coordinates,
                                    'coord_source': 'text'
                                }
                                if is_today:
                                    self.new_warnings_today.append(w_id)
                                    self.captured_warnings_today.append(warning_data)
                                    print(f"      âœ… æ–°è­¦å‘Š [ä»Šæ—¥]: {title[:40]}...")
                                else:
                                    self.new_warnings_history.append(w_id)
                                    self.captured_warnings_history.append(warning_data)
                                    print(f"      âœ… æ–°è­¦å‘Š [æ­·å²]: {title[:40]}...")
                            else:
                                print(f"      â­ï¸ å·²å­˜åœ¨")

                        except Exception as e:
                            print(f"    âš ï¸ è™•ç†é …ç›® {processed_count + 1} æ™‚å‡ºéŒ¯: {e}")

                        processed_count += 1

                    except Exception as e:
                        print(f"    âš ï¸ ç²å–é …ç›®åˆ—è¡¨æ™‚å‡ºéŒ¯: {e}"); break

                print(f"    âœ… {bureau_name} è™•ç†å®Œæˆï¼Œå…± {processed_count} å€‹é …ç›®")
                break

            except Exception as e:
                print(f"  âš ï¸ æŠ“å– {bureau_name} éŒ¯èª¤ (å˜—è©¦ {retry+1}/{max_retries}): {e}")
                if retry == max_retries - 1:
                    print(f"  âŒ {bureau_name} å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                else:
                    time.sleep(3)

    def scrape_all_bureaus(self):
        print(f"\nğŸ‡¨ğŸ‡³ é–‹å§‹çˆ¬å–ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Š...")
        try:
            self.driver.get('https://www.msa.gov.cn/page/outter/weather.jsp')
            time.sleep(5)
            nav_btn = self.wait.until(EC.element_to_be_clickable((By.XPATH, "//span[contains(text(), 'èˆªè¡Œè­¦å‘Š')]")))
            self.driver.execute_script("arguments[0].click();", nav_btn)
            time.sleep(3)

            bureaus = [
                b.text.strip()
                for b in self.driver.find_elements(By.CSS_SELECTOR, ".nav_lv2_list .nav_lv2_text")
                if b.text.strip()
            ]
            print(f"  ğŸ“ æ‰¾åˆ° {len(bureaus)} å€‹æµ·äº‹å±€")

            for b_name in bureaus:
                try:
                    elem = self.driver.find_element(
                        By.XPATH,
                        f"//div[@class='nav_lv2_text' and contains(text(), '{b_name}')]"
                    )
                    self.scrape_bureau_warnings(b_name, elem)
                    time.sleep(1)
                except Exception as e:
                    print(f"    âš ï¸ è·³é {b_name}: {e}"); continue

        except Exception as e:
            print(f"âŒ ä¸­åœ‹æµ·äº‹å±€çˆ¬å–éŒ¯èª¤: {e}"); traceback.print_exc()
        finally:
            try:
                self.driver.quit()
                print("  ğŸ”’ WebDriver å·²é—œé–‰ (ä¸­åœ‹æµ·äº‹å±€)")
            except:
                pass

        total_new = len(self.new_warnings_today) + len(self.new_warnings_history)
        print(f"\nğŸ‡¨ğŸ‡³ ä¸­åœ‹æµ·äº‹å±€çˆ¬å–å®Œæˆ:")
        print(f"   ğŸ†• ä»Šæ—¥æ–°å¢: {len(self.new_warnings_today)} ç­†")
        print(f"   ğŸ“š æ­·å²è³‡æ–™: {len(self.new_warnings_history)} ç­†")
        print(f"   ğŸ“Š ç¸½è¨ˆ: {total_new} ç­†")
        return {'today': self.new_warnings_today, 'history': self.new_warnings_history}


# ==================== 8. ç’°å¢ƒè®Šæ•¸è®€å– ====================
print("ğŸ“‹ æ­£åœ¨è®€å–ç’°å¢ƒè®Šæ•¸...")

TEAMS_WEBHOOK    = os.getenv("TEAMS_WEBHOOK_URL", "")
MAIL_USER        = os.getenv("MAIL_USER", "")
MAIL_PASSWORD    = os.getenv("MAIL_PASSWORD", "")
TARGET_EMAIL     = os.getenv("TARGET_EMAIL", "")
MAIL_SMTP_SERVER = os.getenv("MAIL_SMTP_SERVER", "smtp.gmail.com")
MAIL_SMTP_PORT   = int(os.getenv("MAIL_SMTP_PORT", "587"))
DB_FILE_PATH     = os.getenv("DB_FILE_PATH", "navigation_warnings.db")
BACKUP_DIR       = os.getenv("BACKUP_DIR", "backups")
MAX_BACKUP_FILES = int(os.getenv("MAX_BACKUP_FILES", "7"))
SCRAPE_INTERVAL  = int(os.getenv("SCRAPE_INTERVAL", "3600"))
MAX_RETRIES      = int(os.getenv("MAX_RETRIES", "3"))
REQUEST_TIMEOUT  = int(os.getenv("REQUEST_TIMEOUT", "30"))
KEYWORDS_CONFIG  = os.getenv("KEYWORDS_CONFIG", "keywords_config.json")
CHROME_HEADLESS  = os.getenv("CHROME_HEADLESS", "true").lower() == "true"

ENABLE_EMAIL_NOTIFICATIONS = os.getenv("ENABLE_EMAIL_NOTIFICATIONS", "true").lower() == "true"
ENABLE_TEAMS_NOTIFICATIONS = os.getenv("ENABLE_TEAMS_NOTIFICATIONS", "true").lower() == "true"
ENABLE_CN_MSA              = os.getenv("ENABLE_CN_MSA",  "true").lower() == "true"
ENABLE_TW_MPB              = os.getenv("ENABLE_TW_MPB",  "true").lower() == "true"
ENABLE_UKMTO               = os.getenv("ENABLE_UKMTO",   "true").lower() == "true"
SCRAPE_DAYS                = int(os.getenv("SCRAPE_DAYS",       "3"))
UKMTO_SCRAPE_DAYS          = int(os.getenv("UKMTO_SCRAPE_DAYS", "30"))

print("\n" + "="*70)
print("âš™ï¸  ç³»çµ±è¨­å®šæª¢æŸ¥")
print("="*70)
print(f"ğŸ“§ Email é€šçŸ¥: {'âœ… å•Ÿç”¨' if ENABLE_EMAIL_NOTIFICATIONS and MAIL_USER else 'âŒ åœç”¨'}")
print(f"ğŸ“¢ Teams é€šçŸ¥: {'âœ… å•Ÿç”¨' if ENABLE_TEAMS_NOTIFICATIONS and TEAMS_WEBHOOK else 'âŒ åœç”¨'}")
print(f"ğŸ’¾ è³‡æ–™åº«: {DB_FILE_PATH}")
print(f"ğŸ“… æŠ“å–ç¯„åœ: CN/TW æœ€è¿‘ {SCRAPE_DAYS} å¤© | UKMTO æœ€è¿‘ {UKMTO_SCRAPE_DAYS} å¤©")
print(f"ğŸ” è³‡æ–™ä¾†æº: CN_MSA={'âœ…' if ENABLE_CN_MSA else 'âŒ'} | TW_MPB={'âœ…' if ENABLE_TW_MPB else 'âŒ'} | UKMTO={'âœ…' if ENABLE_UKMTO else 'âŒ'}")
print("="*70 + "\n")


# ==================== 9. ä¸»ç¨‹å¼é€²å…¥é» ====================
if __name__ == "__main__":
    try:
        print("\n" + "="*70)
        print("ğŸŒŠ æµ·äº‹è­¦å‘Šç›£æ§ç³»çµ±å•Ÿå‹• v3.1")
        print("="*70)

        print("\nğŸ“¦ åˆå§‹åŒ–è³‡æ–™åº«...")
        db_manager = DatabaseManager(db_name=DB_FILE_PATH)
        print(f"  âœ… è³‡æ–™åº«åˆå§‹åŒ–æˆåŠŸ: {DB_FILE_PATH}")

        print("ğŸ”‘ åˆå§‹åŒ–é—œéµå­—ç®¡ç†å™¨...")
        keyword_manager = KeywordManager(config_file=KEYWORDS_CONFIG)

        print("ğŸ—ºï¸  åˆå§‹åŒ–åº§æ¨™æå–å™¨...")
        coord_extractor = CoordinateExtractor()

        teams_notifier = None
        if ENABLE_TEAMS_NOTIFICATIONS and TEAMS_WEBHOOK:
            print("ğŸ“¢ åˆå§‹åŒ– Teams é€šçŸ¥å™¨...")
            teams_notifier = UnifiedTeamsNotifier(TEAMS_WEBHOOK)

        email_notifier = None
        if ENABLE_EMAIL_NOTIFICATIONS and all([MAIL_USER, MAIL_PASSWORD, TARGET_EMAIL]):
            print("ğŸ“§ åˆå§‹åŒ– Email é€šçŸ¥å™¨...")
            email_notifier = GmailRelayNotifier(MAIL_USER, MAIL_PASSWORD, TARGET_EMAIL)

        cn_scraper    = None
        tw_scraper    = None
        ukmto_scraper = None

        if ENABLE_CN_MSA:
            print("ğŸ‡¨ğŸ‡³ åˆå§‹åŒ–ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ²...")
            cn_scraper = CNMSANavigationWarningsScraper(
                db_manager=db_manager, keyword_manager=keyword_manager,
                teams_notifier=teams_notifier, coord_extractor=coord_extractor,
                headless=CHROME_HEADLESS, days=SCRAPE_DAYS
            )

        if ENABLE_TW_MPB:
            print("ğŸ‡¹ğŸ‡¼ åˆå§‹åŒ–å°ç£èˆªæ¸¯å±€çˆ¬èŸ²...")
            tw_scraper = TWMaritimePortBureauScraper(
                db_manager=db_manager, keyword_manager=keyword_manager,
                teams_notifier=teams_notifier, coord_extractor=coord_extractor,
                days=SCRAPE_DAYS
            )

        if ENABLE_UKMTO:
            print("ğŸ‡¬ğŸ‡§ åˆå§‹åŒ– UKMTO çˆ¬èŸ²...")
            ukmto_scraper = UKMTOScraper(
                db_manager=db_manager, keyword_manager=keyword_manager,
                teams_notifier=teams_notifier, coord_extractor=coord_extractor,
                days=UKMTO_SCRAPE_DAYS
            )

        print("\n" + "="*70)
        print("âœ… æ‰€æœ‰æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")
        print("="*70)

        # ========== é–‹å§‹çˆ¬å– ==========
        print("\nğŸš€ é–‹å§‹çˆ¬å–æµ·äº‹è­¦å‘Š...")

        all_warnings_today   = []
        all_warnings_history = []
        all_captured_today   = []
        all_captured_history = []

        if cn_scraper:
            cn_result = cn_scraper.scrape_all_bureaus()
            all_warnings_today.extend(cn_result['today'])
            all_warnings_history.extend(cn_result['history'])
            all_captured_today.extend(cn_scraper.captured_warnings_today)
            all_captured_history.extend(cn_scraper.captured_warnings_history)

        if tw_scraper:
            tw_result = tw_scraper.scrape_all_pages()
            all_warnings_today.extend(tw_result['today'])
            all_warnings_history.extend(tw_result['history'])
            all_captured_today.extend(tw_scraper.captured_warnings_today)
            all_captured_history.extend(tw_scraper.captured_warnings_history)

        if ukmto_scraper:
            ukmto_result = ukmto_scraper.scrape()
            all_warnings_today.extend(ukmto_result['today'])
            all_warnings_history.extend(ukmto_result['history'])
            all_captured_today.extend(ukmto_scraper.captured_warnings_today)
            all_captured_history.extend(ukmto_scraper.captured_warnings_history)

        # ========== ç™¼é€é€šçŸ¥ ==========
        total_warnings = len(all_warnings_today) + len(all_warnings_history)

        if total_warnings > 0:
            print(f"\nğŸ“¢ ç™¼ç¾ {total_warnings} å€‹è­¦å‘Š (ä»Šæ—¥ {len(all_warnings_today)} ç­†ï¼Œæ­·å² {len(all_warnings_history)} ç­†)")

            if teams_notifier and ENABLE_TEAMS_NOTIFICATIONS:

                def _to_teams_tuple(w):
                    """å°‡ warning_data dict è½‰ç‚º Teams é€šçŸ¥æ‰€éœ€çš„ tuple æ ¼å¼"""
                    return (
                        w.get('id'),
                        w.get('bureau'),
                        w.get('title'),
                        w.get('link'),
                        w.get('time'),
                        ', '.join(w.get('keywords', [])) if isinstance(w.get('keywords'), list) else w.get('keywords', ''),
                        w.get('details', ''),             # â† åŸæœ¬æ˜¯ ''ï¼Œç¾åœ¨å¸¶å…¥ details
                        json.dumps(w.get('coordinates', []))
                    )

                for src in ["CN_MSA", "TW_MPB", "UKMTO"]:
                    group = [w for w in all_captured_today if w.get('source') == src]
                    if group:
                        print(f"\nğŸ“¤ ç™¼é€ {src} é€šçŸ¥ [ä»Šæ—¥æ–°å¢]...")
                        teams_notifier.send_batch_notification(
                            [_to_teams_tuple(w) for w in group], src, is_today=True
                        )

                for src in ["CN_MSA", "TW_MPB", "UKMTO"]:
                    group = [w for w in all_captured_history if w.get('source') == src]
                    if group:
                        print(f"\nğŸ“¤ ç™¼é€ {src} é€šçŸ¥ [æ­·å²è³‡æ–™]...")
                        teams_notifier.send_batch_notification(
                            [_to_teams_tuple(w) for w in group], src, is_today=False
                        )

            if email_notifier and ENABLE_EMAIL_NOTIFICATIONS:
                print("\nğŸ“§ ç™¼é€ Email é€šçŸ¥...")
                email_notifier.send_trigger_email(all_captured_today, all_captured_history)

        else:
            print("\nâœ… æ²’æœ‰æ–°çš„è­¦å‘Š")

        # ========== åŸ·è¡Œæ‘˜è¦ ==========
        print("\n" + "="*70)
        print("ğŸ“Š åŸ·è¡Œæ‘˜è¦")
        print("="*70)

        for src, icon in [("CN_MSA", "ğŸ‡¨ğŸ‡³ ä¸­åœ‹æµ·äº‹å±€"), ("TW_MPB", "ğŸ‡¹ğŸ‡¼ å°ç£èˆªæ¸¯å±€"), ("UKMTO", "ğŸ‡¬ğŸ‡§ UKMTO")]:
            t_count  = len([w for w in all_captured_today   if w.get('source') == src])
            h_count  = len([w for w in all_captured_history if w.get('source') == src])
            t_coords = sum(len(w.get('coordinates', [])) for w in all_captured_today   if w.get('source') == src)
            h_coords = sum(len(w.get('coordinates', [])) for w in all_captured_history if w.get('source') == src)

            # UKMTO é¡å¤–é¡¯ç¤ºåº§æ¨™ä¾†æºçµ±è¨ˆ
            if src == "UKMTO":
                all_ukmto = [w for w in all_captured_today + all_captured_history if w.get('source') == 'UKMTO']
                nd_count  = len([w for w in all_ukmto if w.get('coord_source') == 'next_data'])
                tx_count  = len([w for w in all_ukmto if w.get('coord_source') == 'text'])
                no_count  = len([w for w in all_ukmto if w.get('coord_source') == 'none'])
                print(f"\n  {icon}:")
                print(f"     ğŸ†• ä»Šæ—¥æ–°å¢: {t_count} ç­† ({t_coords} å€‹åº§æ¨™é»)")
                print(f"     ğŸ“š æ­·å²è³‡æ–™: {h_count} ç­† ({h_coords} å€‹åº§æ¨™é»)")
                print(f"     ğŸ“¡ åº§æ¨™ä¾†æº: __NEXT_DATA__={nd_count} | æ–‡å­—è§£æ={tx_count} | ç„¡åº§æ¨™={no_count}")
            else:
                print(f"\n  {icon}:")
                print(f"     ğŸ†• ä»Šæ—¥æ–°å¢: {t_count} ç­† ({t_coords} å€‹åº§æ¨™é»)")
                print(f"     ğŸ“š æ­·å²è³‡æ–™: {h_count} ç­† ({h_coords} å€‹åº§æ¨™é»)")

        total_coords = sum(len(w.get('coordinates', [])) for w in all_captured_today + all_captured_history)
        print(f"\n  ğŸ“ˆ ç¸½è¨ˆ: {total_warnings} ç­†è­¦å‘Š")
        print(f"  ğŸ“ ç¸½åº§æ¨™é»æ•¸: {total_coords}")

        print("\n" + "="*70)
        db_manager.print_statistics()

        print("\n" + "="*70)
        print("ğŸ‰ ç³»çµ±åŸ·è¡Œå®Œæˆ v3.1")
        print("="*70)

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        traceback.print_exc()

