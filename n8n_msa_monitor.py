#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±ä¸€æµ·äº‹è­¦å‘Šç›£æ§ç³»çµ± (ä¸­åœ‹æµ·äº‹å±€ + å°ç£èˆªæ¸¯å±€)
æ”¯æ´ç¶“ç·¯åº¦æå–ã€Teams é€šçŸ¥ã€Email å ±å‘Š
"""

import platform
import subprocess
import os
import sys
import logging
import warnings
import json
import smtplib
import requests
import traceback
import re
import time
from datetime import datetime, timezone, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import urllib3
from database_manager import DatabaseManager
from keyword_manager import KeywordManager

# åœç”¨è­¦å‘Š
os.environ['WDM_SSL_VERIFY'] = '0'
load_dotenv()
warnings.filterwarnings('ignore')
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
logging.getLogger('selenium').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)

# éŒ¯èª¤éæ¿¾å™¨ (Windows)
if os.name == 'nt':
    class ErrorFilter:
        def __init__(self, stream):
            self.stream = stream
        def write(self, text):
            if any(k in text for k in ['ERROR:net', 'handshake failed', 'DEPRECATED_ENDPOINT']): 
                return
            self.stream.write(text)
        def flush(self): 
            self.stream.flush()
    sys.stderr = ErrorFilter(sys.stderr)


# ==================== 2. åº§æ¨™æå–å™¨ (å¢å¼·ç‰ˆ) ====================
class CoordinateExtractor:
    """æå–æ–‡æœ¬ä¸­çš„ç¶“ç·¯åº¦åº§æ¨™ï¼ˆå¢å¼·ç‰ˆï¼‰"""
    
    def __init__(self):
        # å„ç¨®ç¶“ç·¯åº¦æ ¼å¼çš„æ­£å‰‡è¡¨é”å¼
        self.patterns = [
            # æ ¼å¼1: 18-17.37N 109-22.17E (åº¦-åˆ†.å°æ•¸)
            r'(\d{1,3})-(\d{1,2}\.\d+)\s*([NSnsåŒ—å—])\s+(\d{1,3})-(\d{1,2}\.\d+)\s*([EWewæ±è¥¿])',
            
            # æ ¼å¼2: 18-17N 109-22E (åº¦-åˆ†)
            r'(\d{1,3})-(\d{1,2})\s*([NSnsåŒ—å—])\s+(\d{1,3})-(\d{1,2})\s*([EWewæ±è¥¿])',
            
            # æ ¼å¼3: 25Â°30'N 121Â°20'E
            r'(\d{1,3})[Â°åº¦]\s*(\d{1,2})[\'â€²åˆ†]?\s*([NSnsåŒ—å—])\s+(\d{1,3})[Â°åº¦]\s*(\d{1,2})[\'â€²åˆ†]?\s*([EWewæ±è¥¿])',
            
            # æ ¼å¼4: 25Â°30.5'N 121Â°20.8'E (å«å°æ•¸åˆ†)
            r'(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?\s*([NSnsåŒ—å—])\s+(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?\s*([EWewæ±è¥¿])',
            
            # æ ¼å¼5: N25Â°30' E121Â°20'
            r'([NSnsåŒ—å—])\s*(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?\s+([EWewæ±è¥¿])\s*(\d{1,3})[Â°åº¦]\s*(\d{1,2}\.?\d*)[\'â€²åˆ†]?',
            
            # æ ¼å¼6: 25.5N 121.3E (åé€²åˆ¶åº¦)
            r'(\d{1,3}\.\d+)\s*[Â°åº¦]?\s*([NSnsåŒ—å—])\s+(\d{1,3}\.\d+)\s*[Â°åº¦]?\s*([EWewæ±è¥¿])',
            
            # æ ¼å¼7: åŒ—ç·¯25åº¦30åˆ† æ±ç¶“121åº¦20åˆ†
            r'[åŒ—å—ç·¯]\s*(\d{1,3})\s*åº¦\s*(\d{1,2})\s*åˆ†\s+[æ±è¥¿ç¶“]\s*(\d{1,3})\s*åº¦\s*(\d{1,2})\s*åˆ†',
        ]
        
        print("  ğŸ—ºï¸ åº§æ¨™æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_coordinates(self, text):
        """
        å¾æ–‡æœ¬ä¸­æå–æ‰€æœ‰ç¶“ç·¯åº¦åº§æ¨™
        è¿”å›: [(lat, lon), ...] åˆ—è¡¨ï¼Œåº§æ¨™ç‚ºåé€²åˆ¶åº¦æ•¸
        """
        coordinates = []
        
        if not text:
            return coordinates
        
        # é è™•ç†ï¼šç§»é™¤ä¸­æ–‡é “è™Ÿã€å…¨å½¢é€—è™Ÿç­‰
        text = text.replace('ã€', ' ').replace('ï¼Œ', ' ').replace('ã€‚', ' ')
        
        for pattern in self.patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    coord = self._parse_match(match, pattern)
                    if coord and self._validate_coordinate(coord):
                        coordinates.append(coord)
                except Exception as e:
                    continue
        
        # å»é‡ï¼ˆä¿ç•™å”¯ä¸€åº§æ¨™ï¼Œå®¹è¨±0.01åº¦èª¤å·®ï¼‰
        unique_coords = []
        for coord in coordinates:
            is_duplicate = False
            for existing in unique_coords:
                if abs(coord[0] - existing[0]) < 0.01 and abs(coord[1] - existing[1]) < 0.01:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_coords.append(coord)
        
        return unique_coords
    
    def _parse_match(self, match, pattern):
        """è§£ææ­£å‰‡åŒ¹é…çµæœç‚ºåé€²åˆ¶åº§æ¨™"""
        groups = match.groups()
        
        # æ ¼å¼6: åé€²åˆ¶åº¦æ•¸ (25.5N 121.3E)
        if len(groups) == 4 and '\\.' in pattern and 'degree' not in pattern:
            try:
                lat = float(groups[0])
                lat_dir = groups[1].upper()
                lon = float(groups[2])
                lon_dir = groups[3].upper()
                
                if lat_dir in ['S', 's', 'å—']:
                    lat = -lat
                if lon_dir in ['W', 'w', 'è¥¿']:
                    lon = -lon
                
                return (lat, lon)
            except:
                return None
        
        # æ ¼å¼5: N25Â°30' E121Â°20'
        if len(groups) >= 6 and groups[0] in ['N', 'S', 'n', 's', 'åŒ—', 'å—']:
            try:
                lat_dir = groups[0].upper()
                lat_deg = float(groups[1])
                lat_min = float(groups[2])
                lon_dir = groups[3].upper()
                lon_deg = float(groups[4])
                lon_min = float(groups[5])
                
                lat = lat_deg + lat_min / 60
                lon = lon_deg + lon_min / 60
                
                if lat_dir in ['S', 's', 'å—']:
                    lat = -lat
                if lon_dir in ['W', 'w', 'è¥¿']:
                    lon = -lon
                
                return (lat, lon)
            except:
                return None
        
        # æ ¼å¼1, 2, 3, 4, 7: åº¦åˆ†æ ¼å¼
        if len(groups) >= 6:
            try:
                lat_deg = float(groups[0])
                lat_min = float(groups[1])
                lat_dir = groups[2].upper() if len(groups[2]) > 0 else 'N'
                lon_deg = float(groups[3])
                lon_min = float(groups[4])
                lon_dir = groups[5].upper() if len(groups[5]) > 0 else 'E'
                
                lat = lat_deg + lat_min / 60
                lon = lon_deg + lon_min / 60
                
                if lat_dir in ['S', 's', 'å—']:
                    lat = -lat
                if lon_dir in ['W', 'w', 'è¥¿']:
                    lon = -lon
                
                return (lat, lon)
            except:
                return None
        
        return None
    
    def _validate_coordinate(self, coord):
        """é©—è­‰åº§æ¨™æ˜¯å¦åˆç†"""
        if not coord or len(coord) != 2:
            return False
        
        lat, lon = coord
        
        # ç·¯åº¦ç¯„åœ: -90 åˆ° 90
        if lat < -90 or lat > 90:
            return False
        
        # ç¶“åº¦ç¯„åœ: -180 åˆ° 180
        if lon < -180 or lon > 180:
            return False
        
        # äºå¤ªæµ·åŸŸå¤§è‡´ç¯„åœæª¢æŸ¥
        # ç·¯åº¦: -60Â°N - 60Â°N, ç¶“åº¦: 60Â°E - 180Â°E
        if not (-60 <= lat <= 60 and 60 <= lon <= 180):
            return False
        
        return True
    
    def extract_from_html(self, html_content):
        """
        å¾ HTML å…§å®¹ä¸­æå–åº§æ¨™
        å°ˆé–€è™•ç†æµ·äº‹å±€ç¶²é æ ¼å¼
        """
        try:
            # ä½¿ç”¨ BeautifulSoup è§£æ
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # å°‹æ‰¾ä¸»è¦å…§å®¹å€åŸŸï¼ˆä¸­åœ‹æµ·äº‹å±€æ ¼å¼ï¼‰
            content_div = soup.find('div', {'class': 'text', 'id': 'ch_p'})
            if content_div:
                text = content_div.get_text()
                return self.extract_coordinates(text)
            
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå€åŸŸï¼Œå¾æ•´å€‹å…§å®¹æå–
            return self.extract_coordinates(html_content)
            
        except Exception as e:
            print(f"    âš ï¸ HTML è§£æå¤±æ•—: {e}")
            return []
    
    def format_coordinates(self, coordinates):
        """æ ¼å¼åŒ–åº§æ¨™åˆ—è¡¨ç‚ºå­—ä¸²"""
        if not coordinates:
            return "ç„¡åº§æ¨™è³‡è¨Š"
        
        formatted = []
        for lat, lon in coordinates:
            # åˆ¤æ–·æ–¹å‘
            lat_dir = 'N' if lat >= 0 else 'S'
            lon_dir = 'E' if lon >= 0 else 'W'
            
            formatted.append(f"{abs(lat):.4f}Â°{lat_dir}, {abs(lon):.4f}Â°{lon_dir}")
        
        return " | ".join(formatted)


# ==================== 3. çµ±ä¸€ Teams é€šçŸ¥ç³»çµ± ====================
class UnifiedTeamsNotifier:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url
    
    def _fix_url(self, url, base_domain=""):
        """ä¿®æ­£ URL æ ¼å¼"""
        if not url: 
            return base_domain or "https://www.msa.gov.cn/page/outter/weather.jsp"
        url = url.strip()
        if url.startswith('/'): 
            return f"{base_domain}{url}" if base_domain else f"https://www.msa.gov.cn{url}"
        if url.startswith(('http://', 'https://')): 
            return url
        if url.startswith(('javascript:', '#')): 
            return base_domain or "https://www.msa.gov.cn/page/outter/weather.jsp"
        return f"{base_domain}/{url}" if base_domain else f"https://www.msa.gov.cn/{url}"
    
    def _create_adaptive_card(self, title, body_elements, actions=None):
        """å»ºç«‹ Adaptive Card æ ¼å¼"""
        card_content = {
            "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
            "type": "AdaptiveCard",
            "version": "1.4",
            "body": [
                {
                    "type": "TextBlock", 
                    "text": title, 
                    "weight": "Bolder", 
                    "size": "Large", 
                    "color": "Attention"
                }
            ] + body_elements
        }
        
        if actions:
            card_content["actions"] = actions
        
        return {
            "type": "message",
            "attachments": [{
                "contentType": "application/vnd.microsoft.card.adaptive",
                "contentUrl": None,
                "content": card_content
            }]
        }

    def send_batch_notification(self, warnings_list, source_type="CN_MSA"):
        """ç™¼é€æ‰¹é‡è­¦å‘Šé€šçŸ¥ (å«åº§æ¨™è³‡è¨Š)"""
        if not self.webhook_url or not warnings_list: 
            return False
        
        try:
            # æ ¹æ“šä¾†æºè¨­å®šåœ–ç¤ºå’Œåç¨±
            if source_type == "TW_MPB":
                source_icon = "ğŸ‡¹ğŸ‡¼"
                source_name = "å°ç£èˆªæ¸¯å±€"
                home_url = "https://www.motcmpb.gov.tw/Information/Notice?SiteId=1&NodeId=483"
                base_domain = "https://www.motcmpb.gov.tw"
            else:
                source_icon = "ğŸ‡¨ğŸ‡³"
                source_name = "ä¸­åœ‹æµ·äº‹å±€"
                home_url = "https://www.msa.gov.cn/page/outter/weather.jsp"
                base_domain = "https://www.msa.gov.cn"
            
            body_elements = [
                {
                    "type": "TextBlock", 
                    "text": f"{source_icon} **{source_name}** ç™¼ç¾ **{len(warnings_list)}** å€‹æ–°çš„èˆªè¡Œè­¦å‘Š", 
                    "size": "Medium", 
                    "weight": "Bolder"
                },
                {
                    "type": "TextBlock", 
                    "text": "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”", 
                    "wrap": True
                }
            ]
            
            actions = []
            
            # é¡¯ç¤ºå‰ 8 ç­†
            for idx, w in enumerate(warnings_list[:8], 1):
                _, bureau, title, link, pub_time, _, _, coordinates = w
                fixed_link = self._fix_url(link, base_domain)
                
                # åº§æ¨™æ‘˜è¦
                coord_summary = "ç„¡åº§æ¨™"
                if coordinates:
                    try:
                        coord_list = json.loads(coordinates) if isinstance(coordinates, str) else coordinates
                        if coord_list:
                            coord_summary = f"ğŸ“ {len(coord_list)} å€‹åº§æ¨™é»"
                    except:
                        coord_summary = "åº§æ¨™æ ¼å¼éŒ¯èª¤"
                
                body_elements.extend([
                    {
                        "type": "TextBlock", 
                        "text": f"**{idx}. {bureau}**", 
                        "weight": "Bolder", 
                        "color": "Accent", 
                        "spacing": "Medium"
                    },
                    {
                        "type": "TextBlock", 
                        "text": title[:100], 
                        "wrap": True
                    },
                    {
                        "type": "TextBlock", 
                        "text": f"ğŸ“… {pub_time} | {coord_summary}", 
                        "size": "Small", 
                        "isSubtle": True
                    }
                ])
                
                if len(actions) < 4:
                    actions.append({
                        "type": "Action.OpenUrl", 
                        "title": f"ğŸ“„ å…¬å‘Š {idx}", 
                        "url": fixed_link
                    })

            if len(warnings_list) > 8:
                body_elements.append({
                    "type": "TextBlock", 
                    "text": f"*...é‚„æœ‰ {len(warnings_list)-8} ç­†æœªé¡¯ç¤º*", 
                    "isSubtle": True
                })

            actions.append({
                "type": "Action.OpenUrl", 
                "title": f"ğŸ  {source_name}é¦–é ", 
                "url": home_url
            })
            
            payload = self._create_adaptive_card(
                f"ğŸš¨ {source_name} æ‰¹é‡è­¦å‘Šé€šçŸ¥ ({len(warnings_list)})", 
                body_elements, 
                actions
            )
            
            print(f"  ğŸ“¤ æ­£åœ¨ç™¼é€ Teams é€šçŸ¥åˆ°: {self.webhook_url[:50]}...")
            
            response = requests.post(
                self.webhook_url, 
                json=payload, 
                headers={"Content-Type": "application/json"}, 
                timeout=30,
                verify=False
            )
            
            if response.status_code in [200, 202]:
                print(f"âœ… {source_name} Teams æ‰¹é‡é€šçŸ¥ç™¼é€æˆåŠŸ ({len(warnings_list)} ç­†)")
                return True
            else:
                print(f"âŒ {source_name} Teams æ‰¹é‡é€šçŸ¥å¤±æ•—: HTTP {response.status_code}")
                print(f"   å›æ‡‰å…§å®¹: {response.text[:200]}")
                return False
                
        except requests.exceptions.SSLError as e:
            print(f"âŒ {source_name} Teams SSL éŒ¯èª¤: {e}")
            print(f"   ğŸ’¡ å»ºè­°: æª¢æŸ¥ç¶²è·¯ä»£ç†è¨­å®šæˆ–æ†‘è­‰")
            return False
        except requests.exceptions.Timeout as e:
            print(f"âŒ {source_name} Teams é€£ç·šé€¾æ™‚: {e}")
            return False
        except requests.exceptions.ConnectionError as e:
            print(f"âŒ {source_name} Teams é€£ç·šéŒ¯èª¤: {e}")
            return False
        except Exception as e:
            print(f"âŒ {source_name} Teams æ‰¹é‡ç™¼é€å¤±æ•—: {e}")
            traceback.print_exc()
            return False


# ==================== 4. Email é€šçŸ¥ç³»çµ± ====================
class GmailRelayNotifier:
    """Gmail SMTP éƒµä»¶é€šçŸ¥ç³»çµ±"""
    def __init__(self, mail_user, mail_pass, target_email):
        self.mail_user = mail_user
        self.mail_pass = mail_pass
        self.target_email = target_email
        self.smtp_server = "smtp.gmail.com"
        self.smtp_port = 587
        
        if not all([mail_user, mail_pass, target_email]):
            print("âš ï¸ Email é€šçŸ¥æœªå®Œæ•´è¨­å®š")
            self.enabled = False
        else:
            self.enabled = True
            print("âœ… Email é€šçŸ¥ç³»çµ±å·²å•Ÿç”¨")
    
    def send_trigger_email(self, warnings_data):
        """ç™¼é€è§¸ç™¼éƒµä»¶ï¼ˆå«åº§æ¨™è³‡è¨Šï¼‰"""
        if not self.enabled:
            print("â„¹ï¸ Email é€šçŸ¥æœªå•Ÿç”¨")
            return False
        
        try:
            msg = MIMEMultipart('related')
            msg['Subject'] = f"ğŸŒŠ èˆªè¡Œè­¦å‘Šç›£æ§å ±å‘Š - {(datetime.now(timezone.utc) + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')}(TPE) / {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M')}(UTC)"
            msg['From'] = self.mail_user
            msg['To'] = self.target_email
            
            # ç”Ÿæˆ HTML å…§å®¹
            html_content = self._generate_html_report(warnings_data)
            
            msg_alternative = MIMEMultipart('alternative')
            msg.attach(msg_alternative)
            msg_alternative.attach(MIMEText(html_content, 'html', 'utf-8'))
            
            # ç™¼é€éƒµä»¶
            print(f"ğŸ“§ æ­£åœ¨ç™¼é€éƒµä»¶è‡³ {self.target_email}...")
            
            with smtplib.SMTP(self.smtp_server, self.smtp_port, timeout=30) as server:
                server.starttls()
                server.login(self.mail_user, self.mail_pass)
                server.send_message(msg)
            
            print(f"âœ… éƒµä»¶ç™¼é€æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ éƒµä»¶ç™¼é€å¤±æ•—: {e}")
            traceback.print_exc()
            return False
    
    def _generate_html_report(self, warnings_data):
        """ç”Ÿæˆ HTML å ±å‘Šï¼ˆå«åº§æ¨™è³‡è¨Šï¼‰"""
        coord_extractor = CoordinateExtractor()
        
        html = f"""
        <html>
        <head>
            <meta charset="utf-8">
            <style>
                body {{ font-family: 'Microsoft JhengHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                .container {{ max-width: 1000px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                h1 {{ color: #003366; border-bottom: 3px solid #0066cc; padding-bottom: 10px; }}
                .warning-item {{ background: #f9f9f9; padding: 15px; margin: 15px 0; border-left: 4px solid #0066cc; border-radius: 5px; }}
                .warning-title {{ font-weight: bold; color: #003366; font-size: 16px; }}
                .warning-meta {{ color: #666; font-size: 14px; margin-top: 5px; }}
                .coordinates {{ background: #e3f2fd; padding: 10px; margin-top: 10px; border-radius: 5px; font-family: 'Courier New', monospace; font-size: 13px; }}
                .coord-item {{ margin: 3px 0; }}
                .footer {{ margin-top: 30px; padding-top: 20px; border-top: 1px solid #ddd; color: #666; font-size: 12px; text-align: center; }}
                .source-icon {{ font-size: 20px; }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸŒŠ æµ·äº‹è­¦å‘Šç›£æ§å ±å‘Š</h1>
                <p><strong>å ±å‘Šæ™‚é–“ï¼š</strong>{(datetime.now(timezone.utc) + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M')}(TPE) / {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M')}(UTC)"</p>
                <p><strong>è­¦å‘Šæ•¸é‡ï¼š</strong>{len(warnings_data)} ç­†</p>
                <hr>
        """
        
        for idx, w in enumerate(warnings_data, 1):
            source_icon = "ğŸ‡¹ğŸ‡¼" if w.get('source') == 'TW_MPB' else "ğŸ‡¨ğŸ‡³"
            
            # æ ¼å¼åŒ–åº§æ¨™
            coords = w.get('coordinates', [])
            coord_html = ""
            if coords:
                coord_html = '<div class="coordinates"><strong>ğŸ“ åº§æ¨™è³‡è¨Šï¼š</strong><br>'
                for i, (lat, lon) in enumerate(coords, 1):
                    lat_dir = 'N' if lat >= 0 else 'S'
                    lon_dir = 'E' if lon >= 0 else 'W'
                    coord_html += f'<div class="coord-item">{i}. {abs(lat):.4f}Â°{lat_dir}, {abs(lon):.4f}Â°{lon_dir}</div>'
                coord_html += '</div>'
            
            html += f"""
                <div class="warning-item">
                    <div class="warning-title"><span class="source-icon">{source_icon}</span> {idx}. {w.get('title', 'N/A')}</div>
                    <div class="warning-meta">
                        ğŸ“‹ ç™¼å¸ƒå–®ä½ï¼š{w.get('bureau', 'N/A')}<br>
                        ğŸ“… ç™¼å¸ƒæ™‚é–“ï¼š{w.get('time', 'N/A')}<br>
                        ğŸ”‘ é—œéµå­—ï¼š{', '.join(w.get('keywords', [])) if isinstance(w.get('keywords'), list) else w.get('keywords', 'N/A')}<br>
                        ğŸ”— <a href="{w.get('link', '#')}">æŸ¥çœ‹è©³æƒ…</a>
                    </div>
                    {coord_html}
                </div>
            """
        
        html += """
                <div class="footer">
                    <p>æ­¤ç‚ºè‡ªå‹•ç™¼é€çš„éƒµä»¶ï¼Œè«‹å‹¿ç›´æ¥å›è¦†</p>
                    <p>èˆªè¡Œè­¦å‘Šç›£æ§ç³»çµ± </p>
                    <p>Navigation Warning Monitor System </p>
                </div>
            </div>
        </body>
        </html>
        """
        
        return html


# ==================== 5. å°ç£èˆªæ¸¯å±€çˆ¬èŸ² ====================
class TWMaritimePortBureauScraper:
    def __init__(self, db_manager, keyword_manager, teams_notifier, coord_extractor, days=0):
        self.db_manager = db_manager
        self.keyword_manager = keyword_manager
        self.keywords = keyword_manager.get_keywords()
        self.teams_notifier = teams_notifier
        self.coord_extractor = coord_extractor
        
        self.base_url = "https://www.motcmpb.gov.tw/Information/Notice?SiteId=1&NodeId=483"
        
        self.days = days
        self.cutoff_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        self.new_warnings = []
        self.captured_warnings_data = []
        
        # å®šç¾©è¦æŠ“å–çš„åˆ†é¡
        self.target_categories = {
            '333': 'ç¤™èˆªå…¬å‘Š',
            '334': 'å°„æ“Šå…¬å‘Š'
        }
        
        print(f"  ğŸ“… å°ç£èˆªæ¸¯å±€çˆ¬èŸ²è¨­å®š: åƒ…æŠ“å–ç•¶å¤©è³‡æ–™ ({self.cutoff_date.strftime('%Y-%m-%d')})")
        
        # ========== åˆå§‹åŒ– Selenium WebDriver ==========
        print("  ğŸŒ æ­£åœ¨å•Ÿå‹• Chrome WebDriver (å°ç£èˆªæ¸¯å±€)...")
        
        options = webdriver.ChromeOptions()
        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        prefs = {
            'profile.default_content_setting_values.notifications': 2,
        }
        options.add_experimental_option('prefs', prefs)
        options.add_experimental_option('excludeSwitches', ['enable-logging', 'enable-automation'])
        options.add_experimental_option('useAutomationExtension', False)
        
        try:
            service = Service(ChromeDriverManager().install())
            if platform.system() == 'Windows':
                service.creation_flags = subprocess.CREATE_NO_WINDOW
            
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(60)
            self.wait = WebDriverWait(self.driver, 20)
            print("  âœ… WebDriver å•Ÿå‹•æˆåŠŸ (å°ç£èˆªæ¸¯å±€)")
            
        except Exception as e:
            print(f"  âŒ WebDriver å•Ÿå‹•å¤±æ•—: {e}")
            raise
    
    def check_keywords(self, text):
        """æª¢æŸ¥é—œéµå­—"""
        if not text:
            return []
        
        matched = []
        for k in self.keywords:
            if k.lower() in text.lower():
                matched.append(k)
        
        # é¡å¤–æª¢æŸ¥ç¤™èˆªå’Œå°„æ“Š
        if 'ç¤™èˆª' in text and 'ç¤™èˆª' not in matched:
            matched.append('ç¤™èˆª')
        if 'å°„æ“Š' in text and 'å°„æ“Š' not in matched:
            matched.append('å°„æ“Š')
        
        return matched
    
    def parse_date(self, date_string):
        """è§£ææ—¥æœŸ (æ”¯æ´æ°‘åœ‹å¹´)"""
        try:
            date_string = date_string.strip()
            
            # è™•ç†æ°‘åœ‹å¹´æ ¼å¼ (ä¾‹å¦‚: 114-01-13 æˆ– 2026-01-13)
            date_match = re.match(r'^(\d{2,4})[/-](\d{1,2})[/-](\d{1,2})$', date_string)
            if date_match:
                year = int(date_match.group(1))
                month = int(date_match.group(2))
                day = int(date_match.group(3))
                
                # åˆ¤æ–·æ˜¯æ°‘åœ‹å¹´é‚„æ˜¯è¥¿å…ƒå¹´
                if year < 1000:  # æ°‘åœ‹å¹´
                    year += 1911
                
                return datetime(year, month, day)
            
            return None
        except Exception as e:
            return None
    
    def is_within_date_range(self, date_string):
        """æª¢æŸ¥æ—¥æœŸç¯„åœ"""
        if not date_string:
            return True
        
        parsed_date = self.parse_date(date_string)
        if parsed_date:
            is_valid = parsed_date >= self.cutoff_date
            if not is_valid:
                print(f"          â­ï¸ æ—¥æœŸéèˆŠ: {date_string}")
            return is_valid
        
        return True
    
    def click_category_tab(self, category_id):
        """é»æ“Šåˆ†é¡æ¨™ç±¤"""
        try:
            self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.tabs a"))
            )
            
            if category_id:
                tab_xpath = f"//div[@class='tabs']//a[@data-val='{category_id}']"
                tab = self.wait.until(
                    EC.element_to_be_clickable((By.XPATH, tab_xpath))
                )
            else:
                tab_xpath = "//div[@class='tabs']//a[@class='active']"
                tab = self.driver.find_element(By.XPATH, tab_xpath)
            
            self.driver.execute_script("arguments[0].scrollIntoView(true);", tab)
            time.sleep(0.5)
            self.driver.execute_script("arguments[0].click();", tab)
            print(f"    âœ… å·²é»æ“Šåˆ†é¡æ¨™ç±¤")
            time.sleep(3)
            
            return True
            
        except Exception as e:
            print(f"    âš ï¸ é»æ“Šåˆ†é¡æ¨™ç±¤å¤±æ•—: {e}")
            return False
    
    def get_notices_selenium(self, page=1, base_category_id=None):
        """ä½¿ç”¨ Selenium çˆ¬å–æŒ‡å®šé é¢ï¼ˆå«åº§æ¨™æå–ï¼‰"""
        try:
            category_name = self.target_categories.get(base_category_id, 'å…¨éƒ¨') if base_category_id else 'å…¨éƒ¨'
            print(f"  æ­£åœ¨è«‹æ±‚å°ç£èˆªæ¸¯å±€ [{category_name}] ç¬¬ {page} é ...")
            
            if page == 1:
                print(f"    ğŸŒ è¼‰å…¥ä¸»é é¢...")
                self.driver.get(self.base_url)
                time.sleep(3)
                
                if base_category_id:
                    if not self.click_category_tab(base_category_id):
                        return {'has_data': False, 'notices': [], 'processed': 0}
            else:
                try:
                    next_button = self.wait.until(
                        EC.element_to_be_clickable((By.CSS_SELECTOR, "li.next a"))
                    )
                    self.driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
                    time.sleep(0.5)
                    self.driver.execute_script("arguments[0].click();", next_button)
                    print(f"    âœ… å·²é»æ“Šä¸‹ä¸€é ")
                    time.sleep(3)
                except Exception as e:
                    print(f"    âš ï¸ ç„¡æ³•ç¿»é : {e}")
                    return {'has_data': False, 'notices': [], 'processed': 0}
            
            try:
                self.wait.until(EC.presence_of_element_located((By.ID, "table")))
                self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#table dl")))
                print(f"    âœ… é é¢å…§å®¹è¼‰å…¥å®Œæˆ")
            except Exception as e:
                print(f"    âš ï¸ ç­‰å¾…å…§å®¹è¼‰å…¥è¶…æ™‚: {e}")
                return {'has_data': False, 'notices': [], 'processed': 0}
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            table_div = soup.find('div', id='table')
            
            if not table_div:
                print(f"    âš ï¸ æ‰¾ä¸åˆ° table div")
                return {'has_data': False, 'notices': [], 'processed': 0}
            
            contents_div = table_div.find('div', class_='contents')
            if not contents_div:
                print(f"    âš ï¸ æ‰¾ä¸åˆ° contents div")
                return {'has_data': False, 'notices': [], 'processed': 0}
            
            all_dl_list = contents_div.find_all('dl')
            data_dl_list = [dl for dl in all_dl_list if 'con-title' not in dl.get('class', [])]
            
            print(f"    ğŸ“‹ æ‰¾åˆ° {len(data_dl_list)} å€‹è³‡æ–™åˆ—")
            
            if len(data_dl_list) == 0:
                return {'has_data': False, 'notices': [], 'processed': 0}
            
            processed_count = 0
            
            for idx, dl in enumerate(data_dl_list, 1):
                try:
                    dt_list = dl.find_all('dt')
                    dd = dl.find('dd')
                    
                    if len(dt_list) < 2 or not dd:
                        print(f"    âš ï¸ ç¬¬ {idx} åˆ—çµæ§‹ä¸å®Œæ•´")
                        continue
                    
                    processed_count += 1
                    
                    number = dt_list[0].get_text(strip=True)
                    date = dt_list[1].get_text(strip=True)
                    unit = dt_list[2].get_text(strip=True) if len(dt_list) > 2 else 'å°ç£èˆªæ¸¯å±€'
                    
                    link_tag = dd.find('a')
                    if link_tag:
                        title = link_tag.get_text(strip=True)
                        link = link_tag.get('href', '')
                        
                        if link and not link.startswith('http'):
                            if link.startswith('/'):
                                link = f"https://www.motcmpb.gov.tw{link}"
                            else:
                                link = f"https://www.motcmpb.gov.tw/{link}"
                    else:
                        title = dd.get_text(strip=True)
                        link = ''
                    
                    print(f"    [{idx}] {number} | {date} | {title[:40]}...")
                    
                    if not self.is_within_date_range(date):
                        continue
                    
                    matched_keywords = self.check_keywords(title)
                    if not matched_keywords:
                        print(f"        â­ï¸ ç„¡é—œéµå­—åŒ¹é…")
                        continue
                    
                    print(f"        âœ… é—œéµå­—åŒ¹é…: {', '.join(matched_keywords)}")
                    
                    # ========== æå–åº§æ¨™ ==========
                    print(f"        ğŸ“ æ­£åœ¨æå–åº§æ¨™...")
                    coordinates = []
                    
                    # 1. å¾æ¨™é¡Œæå–
                    title_coords = self.coord_extractor.extract_coordinates(title)
                    if title_coords:
                        coordinates.extend(title_coords)
                        print(f"          âœ… å¾æ¨™é¡Œæå–åˆ° {len(title_coords)} å€‹åº§æ¨™")
                    
                    # 2. å¾é€£çµé é¢æå–ï¼ˆå°ç£èˆªæ¸¯å±€ç‰¹æ®Šè™•ç†ï¼‰
                    if link:
                        try:
                            print(f"          ğŸŒ æ­£åœ¨è¨ªå•è©³ç´°é é¢...")
                            
                            self.driver.execute_script("window.open('');")
                            self.driver.switch_to.window(self.driver.window_handles[1])
                            
                            self.driver.get(link)
                            time.sleep(2)
                            
                            detail_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                            
                            # å°ç£èˆªæ¸¯å±€çš„å…§å®¹å€åŸŸ
                            content_div = (
                                detail_soup.find('div', class_='editor_Content') or
                                detail_soup.find('div', class_='content') or
                                detail_soup.find('div', id='content') or
                                detail_soup.find('article') or
                                detail_soup.find('div', id='container')
                            )
                            
                            if content_div:
                                page_text = content_div.get_text()
                                page_coords = self.coord_extractor.extract_coordinates(page_text)
                                
                                if page_coords:
                                    for pc in page_coords:
                                        if pc not in coordinates:
                                            coordinates.append(pc)
                                    print(f"          âœ… å¾é é¢æå–åˆ° {len(page_coords)} å€‹åº§æ¨™")
                            
                            self.driver.close()
                            self.driver.switch_to.window(self.driver.window_handles[0])
                            time.sleep(1)
                            
                        except Exception as e:
                            print(f"          âš ï¸ ç„¡æ³•å¾ç¶²é æå–åº§æ¨™: {e}")
                            try:
                                if len(self.driver.window_handles) > 1:
                                    self.driver.close()
                                    self.driver.switch_to.window(self.driver.window_handles[0])
                            except:
                                pass
                    
                    if coordinates:
                        print(f"        ğŸ“ ç¸½å…±æå–åˆ° {len(coordinates)} å€‹åº§æ¨™")
                    else:
                        print(f"        â„¹ï¸ æœªæ‰¾åˆ°åº§æ¨™è³‡è¨Š")
                    
                    # å­˜å…¥è³‡æ–™åº«
                    db_data = (
                        unit,
                        title,
                        link,
                        date,
                        ', '.join(matched_keywords),
                        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        coordinates
                    )
                    
                    is_new, w_id = self.db_manager.save_warning(db_data, source_type="TW_MPB")
                    
                    if is_new and w_id:
                        self.new_warnings.append(w_id)
                        self.captured_warnings_data.append({
                            'id': w_id,
                            'bureau': unit,
                            'title': title,
                            'link': link,
                            'time': date,
                            'keywords': matched_keywords,
                            'source': 'TW_MPB',
                            'category': category_name,
                            'coordinates': coordinates
                        })
                        print(f"        ğŸ’¾ æ–°è³‡æ–™å·²å­˜å…¥ (ID: {w_id})")
                    else:
                        print(f"        â„¹ï¸ è³‡æ–™å·²å­˜åœ¨")
                    
                except Exception as e:
                    print(f"    âš ï¸ è™•ç†é …ç›® {idx} æ™‚å‡ºéŒ¯: {e}")
                    traceback.print_exc()
                    continue
            
            print(f"    ğŸ“Š è™•ç† {processed_count} ç­†")
            
            return {
                'has_data': processed_count > 0,
                'notices': [],
                'processed': processed_count
            }
            
        except Exception as e:
            print(f"  âŒ è«‹æ±‚å¤±æ•—: {e}")
            traceback.print_exc()
            return {'has_data': False, 'notices': [], 'processed': 0}
    
    def scrape_all_pages(self, max_pages=3):
        """çˆ¬å–æ‰€æœ‰é é¢"""
        print(f"\nğŸ‡¹ğŸ‡¼ é–‹å§‹çˆ¬å–å°ç£èˆªæ¸¯å±€èˆªè¡Œè­¦å‘Š...")
        print(f"  ğŸŒ ç›®æ¨™ç¶²å€: {self.base_url}")
        
        try:
            for category_id, category_name in self.target_categories.items():
                print(f"\n  ğŸ“‹ çˆ¬å–åˆ†é¡: {category_name} (ID: {category_id})")
                
                for page in range(1, max_pages + 1):
                    result = self.get_notices_selenium(page, category_id)
                    
                    if not result['has_data']:
                        print(f"    ğŸ›‘ ç¬¬ {page} é æ²’æœ‰è³‡æ–™ï¼Œåœæ­¢")
                        break
                    
                    if result['processed'] < 10:
                        print(f"    â„¹ï¸ ç¬¬ {page} é è³‡æ–™è¼ƒå°‘ï¼Œå¯èƒ½å·²æ¥è¿‘æœ€å¾Œä¸€é ")
                    
                    time.sleep(2)
            
        except Exception as e:
            print(f"âŒ å°ç£èˆªæ¸¯å±€çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
        finally:
            try:
                self.driver.quit()
                print("  ğŸ”’ WebDriver å·²é—œé–‰ (å°ç£èˆªæ¸¯å±€)")
            except:
                pass
        
        print(f"\nğŸ‡¹ğŸ‡¼ å°ç£èˆªæ¸¯å±€çˆ¬å–å®Œæˆï¼Œæ–°å¢ {len(self.new_warnings)} ç­†è­¦å‘Š")
        return self.new_warnings


# ==================== 6. ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ² ====================
class CNMSANavigationWarningsScraper:
    def __init__(self, db_manager, keyword_manager, teams_notifier, coord_extractor, headless=True):
        self.db_manager = db_manager
        self.keyword_manager = keyword_manager
        self.keywords = keyword_manager.get_keywords()
        self.teams_notifier = teams_notifier
        self.coord_extractor = coord_extractor
        
        print("ğŸ‡¨ğŸ‡³ åˆå§‹åŒ–ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ²...")
        
        options = webdriver.ChromeOptions()
        if headless:
            options.add_argument('--headless=new')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        prefs = {'profile.managed_default_content_settings.images': 2}
        options.add_experimental_option('prefs', prefs)
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        
        try:
            service = Service(ChromeDriverManager().install())
            if platform.system() == 'Windows':
                service.creation_flags = subprocess.CREATE_NO_WINDOW
            
            self.driver = webdriver.Chrome(service=service, options=options)
            self.driver.set_page_load_timeout(120)
            self.wait = WebDriverWait(self.driver, 15)
            print("  âœ… WebDriver å•Ÿå‹•æˆåŠŸ")
            
        except Exception as e:
            print(f"  âŒ WebDriver å•Ÿå‹•å¤±æ•—: {e}")
            raise
        
        self.three_days_ago = datetime.now() - timedelta(days=3)
        self.new_warnings = []
        self.captured_warnings_data = []
    
    def check_keywords(self, text):
        return [k for k in self.keywords if k.lower() in text.lower()]
    
    def parse_date(self, date_str):
        for fmt in ['%Y-%m-%d', '%Y/%m/%d', '%Yå¹´%mæœˆ%dæ—¥']:
            try: 
                return datetime.strptime(date_str.strip(), fmt)
            except: 
                continue
        return None
    
    def scrape_bureau_warnings(self, bureau_name, bureau_element):
        """æŠ“å–å–®ä¸€æµ·äº‹å±€è­¦å‘Šï¼ˆå«åº§æ¨™æå–ï¼Œä¿®æ­£ Stale Elementï¼‰"""
        print(f"  ğŸ” æŠ“å–: {bureau_name}")
        try:
            self.driver.execute_script("arguments[0].scrollIntoView(true); arguments[0].click();", bureau_element)
            time.sleep(2)
            
            self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "right_main")))
            
            processed_count = 0
            max_items = 100
            
            while processed_count < max_items:
                try:
                    items = self.driver.find_elements(By.CSS_SELECTOR, ".right_main a")
                    
                    if processed_count >= len(items):
                        break
                    
                    item = items[processed_count]
                    
                    try:
                        title = item.get_attribute('title') or item.text.strip()
                        title = re.sub(r'\s*\d{4}-\d{2}-\d{2}\s*$', '', title)
                        if not title:
                            processed_count += 1
                            continue

                        matched = self.check_keywords(title)
                        if not matched:
                            processed_count += 1
                            continue

                        link = item.get_attribute('href') or ''
                        if link.startswith('/'):
                            link = f"https://www.msa.gov.cn{link}"
                        
                        try:
                            publish_time = item.find_element(By.CSS_SELECTOR, ".time").text.strip()
                        except:
                            match = re.search(r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}', item.text)
                            publish_time = match.group() if match else ""

                        if publish_time:
                            p_date = self.parse_date(publish_time)
                            if p_date:
                                today = datetime.now().date()
                                if p_date.date() != today:
                                    print(f"      â­ï¸ éç•¶å¤©æ—¥æœŸ: {publish_time}")
                                    processed_count += 1
                                    continue
                                else:
                                    print(f"      âœ… ç•¶å¤©æ—¥æœŸ: {publish_time}")
                            else:
                                print(f"      âš ï¸ ç„¡æ³•è§£ææ—¥æœŸ: {publish_time}")
                                processed_count += 1
                                continue
                        else:
                            print(f"      âš ï¸ ç„¡æ—¥æœŸè³‡è¨Š")
                            processed_count += 1
                            continue
                        
                        # ========== æå–åº§æ¨™ ==========
                        print(f"    ğŸ“ æ­£åœ¨æå–åº§æ¨™: {title[:40]}...")
                        coordinates = []
                        
                        # å¾æ¨™é¡Œæå–
                        title_coords = self.coord_extractor.extract_coordinates(title)
                        if title_coords:
                            coordinates.extend(title_coords)
                            print(f"      âœ… å¾æ¨™é¡Œæå–åˆ° {len(title_coords)} å€‹åº§æ¨™")
                        
                        # å¾é€£çµé é¢æå–ï¼ˆä¸­åœ‹æµ·äº‹å±€å°ˆç”¨ï¼‰
                        if link and not link.startswith('javascript'):
                            try:
                                self.driver.execute_script("arguments[0].scrollIntoView(true);", item)
                                time.sleep(0.5)
                                self.driver.execute_script("arguments[0].click();", item)
                                time.sleep(2)
                                
                                try:
                                    # ä½¿ç”¨å¢å¼·ç‰ˆ HTML æå–
                                    page_html = self.driver.page_source
                                    page_coords = self.coord_extractor.extract_from_html(page_html)
                                    
                                    if page_coords:
                                        for pc in page_coords:
                                            if pc not in coordinates:
                                                coordinates.append(pc)
                                        print(f"      âœ… å¾é é¢æå–åˆ° {len(page_coords)} å€‹åº§æ¨™")
                                except Exception as e:
                                    print(f"      âš ï¸ é é¢å…§å®¹æå–å¤±æ•—: {e}")
                                
                                self.driver.back()
                                time.sleep(2)
                                self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "right_main")))
                                
                            except Exception as e:
                                print(f"      âš ï¸ ç„¡æ³•å¾ç¶²é æå–åº§æ¨™: {e}")
                                try:
                                    self.driver.back()
                                    time.sleep(2)
                                except:
                                    pass
                        
                        if coordinates:
                            print(f"      ğŸ“ ç¸½å…±æå–åˆ° {len(coordinates)} å€‹åº§æ¨™")
                        else:
                            print(f"      âš ï¸ æœªæ‰¾åˆ°åº§æ¨™è³‡è¨Š")
                        
                        # å­˜å…¥è³‡æ–™åº«
                        db_data = (
                            bureau_name,
                            title,
                            link,
                            publish_time,
                            ', '.join(matched),
                            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            coordinates
                        )
                        
                        is_new, w_id = self.db_manager.save_warning(db_data, source_type="CN_MSA")
                        
                        if is_new and w_id:
                            self.new_warnings.append(w_id)
                            self.captured_warnings_data.append({
                                'id': w_id,
                                'bureau': bureau_name,
                                'title': title,
                                'link': link,
                                'time': publish_time,
                                'keywords': matched,
                                'source': 'CN_MSA',
                                'coordinates': coordinates
                            })
                            print(f"      âœ… æ–°è­¦å‘Š: {title[:40]}...")
                        else:
                            print(f"      â­ï¸ å·²å­˜åœ¨")
                    
                    except Exception as e:
                        print(f"    âš ï¸ è™•ç†é …ç›® {processed_count + 1} æ™‚å‡ºéŒ¯: {e}")
                    
                    processed_count += 1
                    
                except Exception as e:
                    print(f"    âš ï¸ ç²å–é …ç›®åˆ—è¡¨æ™‚å‡ºéŒ¯: {e}")
                    break
            
            print(f"    âœ… {bureau_name} è™•ç†å®Œæˆï¼Œå…±è™•ç† {processed_count} å€‹é …ç›®")
                        
        except Exception as e:
            print(f"  âŒ æŠ“å– {bureau_name} éŒ¯èª¤: {e}")
    
    def scrape_all_bureaus(self):
        """çˆ¬å–æ‰€æœ‰æµ·äº‹å±€"""
        print(f"\nğŸ‡¨ğŸ‡³ é–‹å§‹çˆ¬å–ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Š...")
        
        try:
            print("  ğŸ“¡ æ­£åœ¨è¼‰å…¥ä¸­åœ‹æµ·äº‹å±€ç¶²ç«™...")
            self.driver.get('https://www.msa.gov.cn/page/outter/weather.jsp')
            time.sleep(5)
            
            print("  ğŸ–±ï¸ é»æ“Šèˆªè¡Œè­¦å‘Šé¸é …...")
            nav_btn = self.wait.until(
                EC.element_to_be_clickable((By.XPATH, "//span[contains(text(), 'èˆªè¡Œè­¦å‘Š')]"))
            )
            self.driver.execute_script("arguments[0].click();", nav_btn)
            time.sleep(3)
            
            print("  ğŸ“‹ ç²å–æµ·äº‹å±€åˆ—è¡¨...")
            bureaus = [
                b.text.strip() 
                for b in self.driver.find_elements(By.CSS_SELECTOR, ".nav_lv2_list .nav_lv2_text") 
                if b.text.strip()
            ]
            
            print(f"  ğŸ“ æ‰¾åˆ° {len(bureaus)} å€‹æµ·äº‹å±€")
            
            for b_name in bureaus:
                try:
                    elem = self.driver.find_element(
                        By.XPATH, 
                        f"//div[@class='nav_lv2_text' and contains(text(), '{b_name}')]"
                    )
                    self.scrape_bureau_warnings(b_name, elem)
                    time.sleep(1)
                except Exception as e:
                    print(f"    âš ï¸ è·³é {b_name}: {e}")
                    continue
            
        except Exception as e:
            print(f"âŒ ä¸­åœ‹æµ·äº‹å±€çˆ¬å–éŒ¯èª¤: {e}")
            traceback.print_exc()
        finally:
            try:
                self.driver.quit()
            except:
                pass
        
        print(f"ğŸ‡¨ğŸ‡³ ä¸­åœ‹æµ·äº‹å±€çˆ¬å–å®Œæˆï¼Œæ–°å¢ {len(self.new_warnings)} ç­†è­¦å‘Š")
        return self.new_warnings


# ==================== ç’°å¢ƒè®Šæ•¸è®€å– ====================
print("ğŸ“‹ æ­£åœ¨è®€å–ç’°å¢ƒè®Šæ•¸...")

TEAMS_WEBHOOK = os.getenv("TEAMS_WEBHOOK_URL", "")
MAIL_USER = os.getenv("MAIL_USER", "")
MAIL_PASSWORD = os.getenv("MAIL_PASSWORD", "")
TARGET_EMAIL = os.getenv("TARGET_EMAIL", "")
MAIL_SMTP_SERVER = os.getenv("MAIL_SMTP_SERVER", "smtp.gmail.com")
MAIL_SMTP_PORT = int(os.getenv("MAIL_SMTP_PORT", "587"))

DB_FILE_PATH = os.getenv("DB_FILE_PATH", "navigation_warnings.db")
BACKUP_DIR = os.getenv("BACKUP_DIR", "backups")
MAX_BACKUP_FILES = int(os.getenv("MAX_BACKUP_FILES", "7"))

SCRAPE_INTERVAL = int(os.getenv("SCRAPE_INTERVAL", "3600"))
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
REQUEST_TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "30"))

KEYWORDS_CONFIG = os.getenv("KEYWORDS_CONFIG", "keywords_config.json")
CHROME_HEADLESS = os.getenv("CHROME_HEADLESS", "true").lower() == "true"

ENABLE_EMAIL_NOTIFICATIONS = os.getenv("ENABLE_EMAIL_NOTIFICATIONS", "true").lower() == "true"
ENABLE_TEAMS_NOTIFICATIONS = os.getenv("ENABLE_TEAMS_NOTIFICATIONS", "true").lower() == "true"

ENABLE_CN_MSA = os.getenv("ENABLE_CN_MSA", "true").lower() == "true"
ENABLE_TW_MPB = os.getenv("ENABLE_TW_MPB", "true").lower() == "true"

print("\n" + "="*70)
print("âš™ï¸  ç³»çµ±è¨­å®šæª¢æŸ¥")
print("="*70)
print(f"ğŸ“§ Email é€šçŸ¥: {'âœ… å•Ÿç”¨' if ENABLE_EMAIL_NOTIFICATIONS and MAIL_USER else 'âŒ åœç”¨'}")
print(f"ğŸ“¢ Teams é€šçŸ¥: {'âœ… å•Ÿç”¨' if ENABLE_TEAMS_NOTIFICATIONS and TEAMS_WEBHOOK else 'âŒ åœç”¨'}")
print(f"ğŸ’¾ è³‡æ–™åº«: {DB_FILE_PATH}")
print(f"ğŸ” è³‡æ–™ä¾†æº: CN_MSA={'âœ…' if ENABLE_CN_MSA else 'âŒ'} | TW_MPB={'âœ…' if ENABLE_TW_MPB else 'âŒ'}")
print("="*70 + "\n")


# ==================== 8. ä¸»ç¨‹å¼é€²å…¥é» ====================
if __name__ == "__main__":
    try:
        print("\n" + "="*70)
        print("ğŸŒŠ æµ·äº‹è­¦å‘Šç›£æ§ç³»çµ±å•Ÿå‹•")
        print("="*70)
        
        # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
        print("\nğŸ“¦ åˆå§‹åŒ–è³‡æ–™åº«...")
        db_manager = DatabaseManager(db_name=DB_FILE_PATH)
        print(f"  âœ… è³‡æ–™åº«åˆå§‹åŒ–æˆåŠŸ: {DB_FILE_PATH}")
        
        # åˆå§‹åŒ–é—œéµå­—ç®¡ç†å™¨
        print("ğŸ”‘ åˆå§‹åŒ–é—œéµå­—ç®¡ç†å™¨...")
        keyword_manager = KeywordManager(config_file=KEYWORDS_CONFIG)
        
        # åˆå§‹åŒ–åº§æ¨™æå–å™¨
        print("ğŸ—ºï¸  åˆå§‹åŒ–åº§æ¨™æå–å™¨...")
        coord_extractor = CoordinateExtractor()
        
        # åˆå§‹åŒ– Teams é€šçŸ¥å™¨
        teams_notifier = None
        if ENABLE_TEAMS_NOTIFICATIONS and TEAMS_WEBHOOK:
            print("ğŸ“¢ åˆå§‹åŒ– Teams é€šçŸ¥å™¨...")
            teams_notifier = UnifiedTeamsNotifier(TEAMS_WEBHOOK)
        
        # åˆå§‹åŒ– Email é€šçŸ¥å™¨
        email_notifier = None
        if ENABLE_EMAIL_NOTIFICATIONS and all([MAIL_USER, MAIL_PASSWORD, TARGET_EMAIL]):
            print("ğŸ“§ åˆå§‹åŒ– Email é€šçŸ¥å™¨...")
            email_notifier = GmailRelayNotifier(MAIL_USER, MAIL_PASSWORD, TARGET_EMAIL)
        
        # åˆå§‹åŒ–çˆ¬èŸ²
        cn_scraper = None
        tw_scraper = None
        
        if ENABLE_CN_MSA:
            print("ğŸ‡¨ğŸ‡³ åˆå§‹åŒ–ä¸­åœ‹æµ·äº‹å±€çˆ¬èŸ²...")
            cn_scraper = CNMSANavigationWarningsScraper(
                db_manager=db_manager,
                keyword_manager=keyword_manager,
                teams_notifier=teams_notifier,
                coord_extractor=coord_extractor,
                headless=CHROME_HEADLESS
            )
        
        if ENABLE_TW_MPB:
            print("ğŸ‡¹ğŸ‡¼ åˆå§‹åŒ–å°ç£èˆªæ¸¯å±€çˆ¬èŸ²...")
            tw_scraper = TWMaritimePortBureauScraper(
                db_manager=db_manager,
                keyword_manager=keyword_manager,
                teams_notifier=teams_notifier,
                coord_extractor=coord_extractor,
                days=3
            )
        
        print("\n" + "="*70)
        print("âœ… æ‰€æœ‰æ¨¡çµ„åˆå§‹åŒ–å®Œæˆ")
        print("="*70)
        
        # ========== é–‹å§‹çˆ¬å– ==========
        print("\nğŸš€ é–‹å§‹çˆ¬å–æµ·äº‹è­¦å‘Š...")
        
        all_new_warnings = []
        all_captured_data = []
        
        # çˆ¬å–ä¸­åœ‹æµ·äº‹å±€
        if cn_scraper:
            print("\nğŸ‡¨ğŸ‡³ çˆ¬å–ä¸­åœ‹æµ·äº‹å±€...")
            cn_warnings = cn_scraper.scrape_all_bureaus()
            all_new_warnings.extend(cn_warnings)
            all_captured_data.extend(cn_scraper.captured_warnings_data)
        
        # çˆ¬å–å°ç£èˆªæ¸¯å±€
        if tw_scraper:
            print("\nğŸ‡¹ğŸ‡¼ çˆ¬å–å°ç£èˆªæ¸¯å±€...")
            tw_warnings = tw_scraper.scrape_all_pages()
            all_new_warnings.extend(tw_warnings)
            all_captured_data.extend(tw_scraper.captured_warnings_data)
        
        # ========== ç™¼é€é€šçŸ¥ ==========
        if all_new_warnings:
            print(f"\nğŸ“¢ ç™¼ç¾ {len(all_new_warnings)} å€‹æ–°è­¦å‘Šï¼Œæº–å‚™ç™¼é€é€šçŸ¥...")
            
            # Teams é€šçŸ¥
            if teams_notifier and ENABLE_TEAMS_NOTIFICATIONS:
                # åˆ†åˆ¥ç™¼é€ä¸­åœ‹å’Œå°ç£çš„è­¦å‘Š
                cn_warnings_data = [w for w in all_captured_data if w.get('source') == 'CN_MSA']
                tw_warnings_data = [w for w in all_captured_data if w.get('source') == 'TW_MPB']
                
                if cn_warnings_data:
                    print("\nğŸ“¤ ç™¼é€ä¸­åœ‹æµ·äº‹å±€é€šçŸ¥...")
                    cn_list = [(
                        w.get('id'),
                        w.get('bureau'),
                        w.get('title'),
                        w.get('link'),
                        w.get('time'),
                        ', '.join(w.get('keywords', [])) if isinstance(w.get('keywords'), list) else w.get('keywords', ''),
                        '',
                        json.dumps(w.get('coordinates', []))
                    ) for w in cn_warnings_data]
                    teams_notifier.send_batch_notification(cn_list, "CN_MSA")
                
                if tw_warnings_data:
                    print("\nğŸ“¤ ç™¼é€å°ç£èˆªæ¸¯å±€é€šçŸ¥...")
                    tw_list = [(
                        w.get('id'),
                        w.get('bureau'),
                        w.get('title'),
                        w.get('link'),
                        w.get('time'),
                        ', '.join(w.get('keywords', [])) if isinstance(w.get('keywords'), list) else w.get('keywords', ''),
                        '',
                        json.dumps(w.get('coordinates', []))
                    ) for w in tw_warnings_data]
                    teams_notifier.send_batch_notification(tw_list, "TW_MPB")
            
            # Email é€šçŸ¥
            if email_notifier and ENABLE_EMAIL_NOTIFICATIONS:
                print("\nğŸ“§ ç™¼é€ Email é€šçŸ¥...")
                email_notifier.send_trigger_email(all_captured_data)
        else:
            print("\nâœ… æ²’æœ‰æ–°çš„è­¦å‘Š")
        
        # ========== ç”Ÿæˆæ‘˜è¦ ==========
        print("\n" + "="*70)
        print("ğŸ“Š åŸ·è¡Œæ‘˜è¦")
        print("="*70)
        
        cn_count = len([w for w in all_captured_data if w.get('source') == 'CN_MSA'])
        tw_count = len([w for w in all_captured_data if w.get('source') == 'TW_MPB'])
        total_coords = sum(len(w.get('coordinates', [])) for w in all_captured_data)
        
        print(f"ğŸ‡¨ğŸ‡³ ä¸­åœ‹æµ·äº‹å±€: {cn_count} ç­†æ–°è­¦å‘Š")
        print(f"ğŸ‡¹ğŸ‡¼ å°ç£èˆªæ¸¯å±€: {tw_count} ç­†æ–°è­¦å‘Š")
        print(f"ğŸ“ ç¸½åº§æ¨™é»æ•¸: {total_coords}")
        
        # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
        print("\n" + "="*70)
        db_manager.print_statistics()
        
        print("\n" + "="*70)
        print("ğŸ‰ ç³»çµ±åŸ·è¡Œå®Œæˆ")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        traceback.print_exc()
